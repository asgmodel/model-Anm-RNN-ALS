{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJRfbwK30yiN"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "# @title Load Data\n",
        "\n",
        "\n",
        "path='/content/DS-ALS-REDSEA-CLEANED.csv'  # @param {type: \"string\"}\n",
        "df = pd.read_csv(path)\n",
        "# @title PreProcessing Raw Data\n",
        "\n",
        "Timestamp=df['Timestamp']\n",
        "Speed=df['Speed']\n",
        "Course=df['Course']\n",
        "Latitude=df['Latitude']\n",
        "Longitude=df['Longitude']\n",
        "Vessel=df['Vessel']\n",
        "new_df=pd.DataFrame({'Timestamp':Timestamp,'Speed':Speed,'Course':Course,'Latitude':Latitude,'Longitude':Longitude,'Vessel':Vessel})\n",
        "\n",
        "#  REMOVE  VALUE masked     IN TABLE\n",
        "df_cleaned = new_df.dropna()\n",
        "\n",
        "df_cleaned = df_cleaned.replace('masked', pd.NA)\n",
        "df_cleaned = df_cleaned.dropna()\n",
        "df_cleaned['Latitude'] = df_cleaned['Latitude'].astype(float)\n",
        "df_cleaned['Longitude'] = df_cleaned['Longitude'].astype(float)\n",
        "df_cleaned['Speed'] = df_cleaned['Speed'].astype(float)\n",
        "df_cleaned['Course']=df_cleaned['Course'].astype(float)\n",
        "\n",
        "# Filter data based on latitude and longitude ranges\n",
        "df_cleaned = df_cleaned[(df_cleaned['Longitude'] >= 32) & (df_cleaned['Longitude'] <= 44)]\n",
        "df_cleaned = df_cleaned[(df_cleaned['Latitude'] >= 12) & (df_cleaned['Latitude'] <= 33)]\n",
        "\n",
        "df_cleaned.sort_values(by=['Timestamp'], inplace=True)\n",
        "df_cleaned.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "vs=df_cleaned['Vessel'].unique()\n",
        "\n",
        "# Group data by vessel and count the number of points in each trajectory\n",
        "vessel_counts = df_cleaned.groupby('Vessel').size()\n",
        "\n",
        "# Filter out vessels with more than 100 points\n",
        "vessels_to_keep = vessel_counts[vessel_counts <= 150].index\n",
        "\n",
        "# Filter the DataFrame to keep only the trajectories with 100 or fewer points\n",
        "df_filtered = df_cleaned[df_cleaned['Vessel'].isin(vessels_to_keep)]\n",
        "\n",
        "# Find min/max latitude and longitude for the filtered data\n",
        "min_lat = df_filtered['Latitude'].min()\n",
        "max_lat = df_filtered['Latitude'].max()\n",
        "min_lon = df_filtered['Longitude'].min()\n",
        "max_lon = df_filtered['Longitude'].max()\n",
        "min_time=df_filtered['Timestamp'].min()\n",
        "max_time=df_filtered['Timestamp'].max()\n",
        "max_speed=df_filtered['Speed'].max()\n",
        "min_speed=df_filtered['Speed'].min()\n",
        "min_course=df_filtered['Course'].min()\n",
        "\n",
        "max_course=df_filtered['Course'].max()\n",
        "\n",
        "\n",
        "print(f\"Min Latitude: {min_lat}\")\n",
        "print(f\"Max Latitude: {max_lat}\")\n",
        "print(f\"Min Longitude: {min_lon}\")\n",
        "print(f\"Max Longitude: {max_lon}\")\n",
        "print(f\"Min Timestamp: {min_time}\")\n",
        "print(f\"Max Timestamp: {max_time}\")\n",
        "print(f\"Min Speed: {min_speed}\")\n",
        "print(f\"Max Speed: {max_speed}\")\n",
        "print(f\"Min Course: {min_course}\")\n",
        "print(f\"Max Course: {max_course}\")\n",
        "df_cleaned=df_filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXKAB0NT2j_Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WZdePAWj3eu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbtiRuHUSzG5"
      },
      "outputs": [],
      "source": [
        "# @title Four-Hot\n",
        "\n",
        "LAT_BINS = 100  # @param {type: \"number\"}\n",
        "LON_BINS = 100  # @param {type: \"number\"}\n",
        "SOG_BINS = 50  # @param {type: \"number\"}\n",
        "COG_BINS = 10  # @param {type: \"number\"}\n",
        "\n",
        "data=[]\n",
        "labels=[]\n",
        "features=['Latitude','Longitude','Speed','Course']\n",
        "for v in vs:\n",
        "  v1=df_cleaned[df_cleaned['Vessel']==v]\n",
        "  fs=v1[features].values\n",
        "\n",
        "  fs=torch.tensor(fs)\n",
        "  fs[:,0]=fs[:,0]/LAT_BINS\n",
        "  fs[:,1]=fs[:,1]/LON_BINS\n",
        "  fs[:,2]=fs[:,2]/SOG_BINS\n",
        "  fs[:,3]=fs[:,3]/360\n",
        "  if len(fs)>2:\n",
        "      data.append(fs)\n",
        "\n",
        "Double_DATA=1   # @param {type: \"number\"}\n",
        "data=data*Double_DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxuUI7micSQu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPSKwfbBBzs7"
      },
      "outputs": [],
      "source": [
        "# @title AISDataset\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import concurrent.futures\n",
        "\n",
        "LAT, LON, SOG, COG, HEADING, TIMESTAMP = list(range(6))\n",
        "\n",
        "class AISDataset(Dataset):\n",
        "    def __init__(self, tracks, lat_bins, lon_bins, sog_bins, cog_bins,mean,divm=360):\n",
        "        self.tracks = tracks  #\n",
        "        self.lat_bins = lat_bins\n",
        "        self.lon_bins = lon_bins\n",
        "        self.sog_bins = sog_bins\n",
        "        self.cog_bins = cog_bins\n",
        "        self.total_bins = lat_bins + lon_bins + sog_bins + cog_bins  #\n",
        "\n",
        "        # Normalize the mean values for inputs.\n",
        "        self.mean = mean#self.calculate_mean()\n",
        "        self.dataFourHot=[]\n",
        "        self.dataTrackdFourHot=[]\n",
        "        self.processdata(divm)\n",
        "\n",
        "\n",
        "    def processdata(self,divm=360):\n",
        "       outdata=self.get_four_hot_all_data(divm=divm)\n",
        "       self.dataTrackdFourHot=outdata\n",
        "       print('finsh get_four_hot_all_data',len(outdata))\n",
        "       self.dataFourHot=[]\n",
        "       for  i in range(len(outdata)):\n",
        "         for j in range(len(outdata[i])):\n",
        "           self.dataFourHot.append(outdata[i][j,:])\n",
        "\n",
        "       print('finsh processdata')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_four_hot_all_data(self, divm=360):\n",
        "      results = []\n",
        "      for i in range(len(self.tracks)):\n",
        "              results.append(self.sparse_AIS_to_dense(self.tracks[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      return results\n",
        "\n",
        "    def sparse_AIS_to_dense_all(self,msgs_,divm=360):\n",
        "        msgs_=msgs_/divm\n",
        "        def create_dense_vect(msg, lat_bins=LAT_BINS, lon_bins=LON_BINS, sog_bins=SOG_BINS, cog_bins=COG_BINS):\n",
        "            lat, lon, sog, cog = msg[0], msg[1], msg[2], msg[3]\n",
        "            data_dim = lat_bins + lon_bins + sog_bins + cog_bins\n",
        "            dense_vect = np.zeros(data_dim)\n",
        "            dense_vect[int(lat * lat_bins)] = 1.0\n",
        "            dense_vect[int(lon * lon_bins) + lat_bins] = 1.0\n",
        "            dense_vect[int(sog * sog_bins) + lat_bins + lon_bins] = 1.0\n",
        "            dense_vect[int(cog * cog_bins) + lat_bins + lon_bins + sog_bins] = 1.0\n",
        "            return dense_vect\n",
        "\n",
        "        #\n",
        "        msgs_[msgs_ == 1] = 0.99999\n",
        "\n",
        "        dense_msgs = []\n",
        "        for msg in msgs_:\n",
        "            dense_msgs.append(create_dense_vect(msg))\n",
        "\n",
        "        dense_msgs = np.array(dense_msgs)\n",
        "        return dense_msgs\n",
        "\n",
        "    def sparse_AIS_to_dense_ones(self, msg):\n",
        "        dense_vect = np.zeros(self.total_bins)\n",
        "        dense_vect[int(msg[LAT] * self.lat_bins)] = 1.0\n",
        "        dense_vect[int(msg[LON] * self.lon_bins) + self.lat_bins] = 1.0\n",
        "        dense_vect[int(msg[SOG] * self.sog_bins) + self.lat_bins + self.lon_bins] = 1.0\n",
        "        dense_vect[int(msg[COG] * self.cog_bins) + self.lat_bins + self.lon_bins + self.sog_bins] = 1.0\n",
        "        return dense_vect\n",
        "    def sparse_AIS_to_dense(self, msg_):\n",
        "        dense_vect = np.zeros((len(msg_),self.total_bins))\n",
        "\n",
        "        for i in range(len(msg_)):\n",
        "          msg=msg_[i]\n",
        "\n",
        "         # print(msg)\n",
        "          dense_vect[i,np.int16(msg[0]*self.lat_bins)] = 1.0\n",
        "          dense_vect[i,np.int16(msg[1]* self.lon_bins ) + self.lat_bins] = 1.0\n",
        "          dense_vect[i,np.int16(msg[2]*self.sog_bins) + self.lat_bins + self.lon_bins] = 1.0\n",
        "          dense_vect[i,np.int16(msg[3]* self.cog_bins) + self.lat_bins + self.lon_bins + self.sog_bins] = 1.0\n",
        "        return dense_vect\n",
        "\n",
        "    def dense_to_sparse(self, dense_msg, divm=360):\n",
        "    # Initialize an empty list to store the sparse messages\n",
        "      sparse_msgs = []\n",
        "\n",
        "      # Iterate over the dense message (which represents four-hot encoded data for lat, lon, sog, cog)\n",
        "      for msg in dense_msg:\n",
        "          # Initialize variables to store lat, lon, sog, cog\n",
        "          lat = -1\n",
        "          lon = -1\n",
        "          sog = -1\n",
        "          cog = -1\n",
        "\n",
        "          # Get lat (search in the first section of the vector)\n",
        "          lat_bin = np.argmax(msg[:self.lat_bins])\n",
        "          lat = lat_bin / self.lat_bins\n",
        "\n",
        "          # Get lon (search in the second section of the vector)\n",
        "          lon_bin = np.argmax(msg[self.lat_bins:self.lat_bins + self.lon_bins]) + self.lat_bins\n",
        "          lon = (lon_bin - self.lat_bins) / self.lon_bins\n",
        "\n",
        "          # Get sog (search in the third section of the vector)\n",
        "          sog_bin = np.argmax(msg[self.lat_bins + self.lon_bins:self.lat_bins + self.lon_bins + self.sog_bins]) + self.lat_bins + self.lon_bins\n",
        "          sog = (sog_bin - self.lat_bins - self.lon_bins) / self.sog_bins\n",
        "\n",
        "          # Get cog (search in the fourth section of the vector)\n",
        "          cog_bin = np.argmax(msg[self.lat_bins + self.lon_bins + self.sog_bins:]) + self.lat_bins + self.lon_bins + self.sog_bins\n",
        "          cog = (cog_bin - self.lat_bins - self.lon_bins - self.sog_bins)/ self.cog_bins\n",
        "\n",
        "          # Append the result to sparse_msgs\n",
        "          sparse_msgs.append([lat, lon, sog, cog])\n",
        "\n",
        "      # Convert the sparse_msgs list to a numpy array\n",
        "      sparse_msgs = np.array(sparse_msgs)\n",
        "\n",
        "      # If needed, scale back the values using the divm (scaling factor)\n",
        "      # sparse_msgs *= divm\n",
        "\n",
        "      return sparse_msgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataTrackdFourHot)  #\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "          idx=-1\n",
        "\n",
        "\n",
        "        inputs =self.dataTrackdFourHot[idx]#-self.mean\n",
        "        # return inputs\n",
        "\n",
        "        # Normalize inputs by subtracting mean\n",
        "\n",
        "\n",
        "        return torch.FloatTensor(inputs)\n",
        "def create_dataloader(tracks, batch_size, lat_bins, lon_bins, sog_bins, cog_bins, shuffle=True):\n",
        "    dataset = AISDataset(tracks, lat_bins, lon_bins, sog_bins, cog_bins)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cc7Fw6dcTOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl1N6derUBK4",
        "outputId": "034c57bd-afb7-4d68-e612-9ab93f31247e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finsh get_four_hot_all_data 361\n",
            "finsh processdata\n",
            "Size of dataset: 361\n",
            "Total bins: 260\n",
            "num of message: 21351\n"
          ]
        }
      ],
      "source": [
        "# @title create  dataset\n",
        "\n",
        "batch_size=32  # @param {type: \"number\"}\n",
        "lat_bins=LAT_BINS\n",
        "lon_bins=LON_BINS\n",
        "sog_bins=SOG_BINS\n",
        "\n",
        "\n",
        "mean=[]\n",
        "dataset = AISDataset(data, lat_bins, lon_bins, sog_bins, COG_BINS,mean)\n",
        "print(f\"Size of dataset: {len(dataset)}\")\n",
        "print(f\"Total bins: {dataset.total_bins}\")\n",
        "print(f\"num of message: {len(dataset.dataFourHot)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_TT6Erga2Gi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sfy77PIa2u7"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRmZQdZI2IgQ"
      },
      "outputs": [],
      "source": [
        "# @title VRNNAnomalyQuality\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal, Distribution\n",
        "import math\n",
        "from torch import Tensor\n",
        "# Define Reparameterized Diagonal Gaussian Distribution\n",
        "class ReparameterizedDiagonalGaussian(Distribution):\n",
        "    def __init__(self, mu, log_sigma):\n",
        "        assert mu.shape == log_sigma.shape, \"Mu and log_sigma shapes must match.\"\n",
        "        self.mu = mu\n",
        "        self.sigma = log_sigma.exp()\n",
        "\n",
        "    def sample_epsilon(self):\n",
        "        return torch.empty_like(self.mu).normal_()\n",
        "\n",
        "    def rsample(self):\n",
        "        return self.mu + self.sigma * self.sample_epsilon()\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        dist = Normal(self.mu, self.sigma)\n",
        "        return dist.log_prob(z)\n",
        "\n",
        "\n",
        "def kl_divergence_diag_gaussians(p: ReparameterizedDiagonalGaussian, q: ReparameterizedDiagonalGaussian) -> Tensor:\n",
        "    log_var_p = p.sigma.log() * 2\n",
        "    log_var_q = q.sigma.log() * 2\n",
        "\n",
        "    kl_div = 0.5 * (\n",
        "        (log_var_q - log_var_p).sum(dim=-1)\n",
        "        + ((p.sigma ** 2 + (p.mu - q.mu) ** 2) / (q.sigma ** 2)).sum(dim=-1)\n",
        "        - p.mu.size(-1)\n",
        "    )\n",
        "    return kl_div\n",
        "\n",
        "# Define Encoder, Prior, and Decoder classes as before\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.phi_x = nn.Sequential(nn.Linear(input_dim, latent_dim), nn.ReLU())\n",
        "        self.encoder_net = nn.Linear(latent_dim * 2, 2 * latent_dim)\n",
        "\n",
        "    def forward(self, x_enc, h):\n",
        "        x_enc = self.phi_x(x_enc)\n",
        "        enc = self.encoder_net(torch.cat([x_enc, h[0]], dim=-1))\n",
        "        mu, log_sigma = torch.chunk(enc, 2, dim=-1)\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma), x_enc\n",
        "\n",
        "\n",
        "class Prior(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Prior, self).__init__()\n",
        "        self.prior_net = nn.Linear(latent_dim, 2 * latent_dim)\n",
        "\n",
        "    def forward(self, h):\n",
        "        hidden = self.prior_net(h[0])\n",
        "        mu, log_sigma = torch.chunk(hidden, 2, dim=-1)\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.phi_z = nn.Sequential(nn.Linear(latent_dim, latent_dim), nn.ReLU())\n",
        "        self.decoder_net = nn.Linear(latent_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, z, h):\n",
        "        z_enc = self.phi_z(z)\n",
        "        dec_input = torch.cat([z_enc, h[0]], dim=-1)\n",
        "        logits = self.decoder_net(dec_input)\n",
        "        return torch.sigmoid(logits), z_enc\n",
        "\n",
        "\n",
        "# Define GRUState and LSTMState classes\n",
        "class GRUState(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(GRUState, self).__init__()\n",
        "        self.gru = nn.GRU(latent_dim * 2, latent_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x_enc, z_enc, h):\n",
        "        gru_input = torch.cat([x_enc, z_enc], dim=-1).unsqueeze(1)\n",
        "        _, h_next = self.gru(gru_input, h.unsqueeze(0))\n",
        "        return h_next.squeeze(0), None  # No cell state needed\n",
        "\n",
        "\n",
        "class LSTMState(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(LSTMState, self).__init__()\n",
        "        self.lstm = nn.LSTMCell(latent_dim * 2, latent_dim)\n",
        "\n",
        "    def forward(self, x_enc, z_enc, h, c):\n",
        "        lstm_input = torch.cat([x_enc, z_enc], dim=-1)\n",
        "        h_next, c_next = self.lstm(lstm_input, (h, c))\n",
        "        return h_next, c_next\n",
        "\n",
        "\n",
        "# Define main VRNN model with a flexible state update mechanism\n",
        "class VRNNAnomalyQuality_T(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, state_type=\"LSTM\"):\n",
        "        super(VRNNAnomalyQuality, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(input_dim, latent_dim)\n",
        "        self.prior = Prior(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim, input_dim)\n",
        "\n",
        "        # Choose the state mechanism\n",
        "        if state_type == \"GRU\":\n",
        "            self.state_update = GRUState(latent_dim)\n",
        "        elif state_type == \"LSTM\":\n",
        "            self.state_update = LSTMState(latent_dim)\n",
        "        else:\n",
        "            raise ValueError(\"state_type must be either 'GRU' or 'LSTM'\")\n",
        "\n",
        "        self.state_type = state_type\n",
        "        self.h_0 = torch.zeros(1, latent_dim)\n",
        "        self.c_0 = torch.zeros(1, latent_dim) if state_type == \"LSTM\" else None\n",
        "        self.threshold = 0.0001\n",
        "\n",
        "    def forward(self, x, beta=1.0):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h = self.h_0.expand(batch_size, -1).to(x.device)\n",
        "        c = self.c_0.expand(batch_size, -1).to(x.device) if self.c_0 is not None else None\n",
        "        listlogis=[]\n",
        "\n",
        "        total_loss, kl_divergence, recon_loss = 0, 0, 0\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            prior_dist = self.prior((h, c))\n",
        "            posterior_dist, x_enc = self.encoder(x_t, (h, c))\n",
        "\n",
        "            z_t = posterior_dist.rsample()\n",
        "            x_recon, z_enc = self.decoder(z_t, (h, c))\n",
        "\n",
        "\n",
        "            # Update state based on chosen state mechanism\n",
        "            if self.state_type == \"LSTM\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h, _ = self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "\n",
        "\n",
        "            kl_div = kl_divergence_diag_gaussians(posterior_dist, prior_dist).sum(dim=-1)\n",
        "            kl_divergence += kl_div.mean()\n",
        "            x_recon, z_enc = self.decoder(z_t, (h, c))\n",
        "            recon_loss += nn.functional.binary_cross_entropy(x_recon, x_t, reduction='sum')\n",
        "            listlogis.append(x_recon)\n",
        "            # total_loss = recon_loss -beta * kl_divergence\n",
        "\n",
        "        recon_loss /= seq_len\n",
        "        kl_divergence /= seq_len\n",
        "        total_loss = recon_loss + beta * kl_divergence\n",
        "        return total_loss, recon_loss, kl_divergence\n",
        "    def get_logis(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h = self.h_0.expand(batch_size, -1).to(x.device)\n",
        "        c = self.c_0.expand(batch_size, -1).to(x.device) if self.c_0 is not None else None\n",
        "        listlogis=[]\n",
        "\n",
        "        total_loss, kl_divergence, recon_loss = 0, 0, 0\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            prior_dist = self.prior((h, c))\n",
        "            posterior_dist, x_enc = self.encoder(x_t, (h, c))\n",
        "            z_t = posterior_dist.rsample()\n",
        "            x_recon, z_enc = self.decoder(z_t, (h, c))\n",
        "            listlogis.append(x_recon)\n",
        "            if self.state_type == \"LSTM\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h, _ = self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "        x_rect=torch.stack(listlogis,dim=0)\n",
        "        x_rect=x_rect.permute(1, 0, 2)\n",
        "        return x_rect\n",
        "    def calculate_anomaly_rate(self, inputs):\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "        h = self.h_0.expand(batch_size, -1).contiguous().to(inputs.device)\n",
        "\n",
        "        if self.state_type == 'LSTM':\n",
        "            c = self.c_0.expand(batch_size, -1).contiguous().to(inputs.device)\n",
        "\n",
        "        total_anomalies = 0\n",
        "        total_points = 0\n",
        "\n",
        "        for t in range(inputs.size(1)):\n",
        "            x = inputs[:, t, :]\n",
        "\n",
        "            posterior_dist, x_enc = self.encoder(x, (h, c) if self.state_type == 'LSTM' else (h,None))\n",
        "            z = posterior_dist.rsample()\n",
        "\n",
        "            x_recon, z_enc = self.decoder(z, (h, c) if self.state_type == 'LSTM' else(h,None))\n",
        "\n",
        "            diff = torch.abs(x - x_recon)\n",
        "            anomalies = (diff > self.threshold).sum(dim=-1)  #\n",
        "\n",
        "            total_anomalies += anomalies.sum().item()\n",
        "            total_points += x.numel()\n",
        "\n",
        "            if self.state_type == 'LSTM':\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h ,_= self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "        # حساب نسبة الشذوذ\n",
        "        anomaly_rate = total_anomalies / total_points\n",
        "        return anomaly_rate\n",
        "    def calc_mi(self, inputs):\n",
        "        \"\"\"\n",
        "        Calculate mutual information (MI) for the VRNN model with support for both GRU and LSTM states.\n",
        "        \"\"\"\n",
        "        batch_size = inputs.size(0)\n",
        "        h = self.h_0.expand(batch_size, -1).contiguous().to(inputs.device)\n",
        "\n",
        "        if self.state_type == 'LSTM':\n",
        "            c = self.c_0.expand(batch_size, -1).contiguous().to(inputs.device)\n",
        "\n",
        "        neg_entropy = 0\n",
        "        log_qz = 0\n",
        "\n",
        "        for t in range(inputs.size(1)):\n",
        "            x = inputs[:, t, :]\n",
        "\n",
        "            # Posterior and prior distributions\n",
        "            posterior_dist, x_enc = self.encoder(x, (h, c) if self.state_type == 'LSTM' else (h,None))\n",
        "            pz = self.prior((h, c) if self.state_type == 'LSTM' else h)\n",
        "\n",
        "            # Sample from posterior\n",
        "            mu = posterior_dist.mu\n",
        "            logsigma = torch.log(posterior_dist.sigma)\n",
        "            z = posterior_dist.rsample()\n",
        "            _, z_enc = self.decoder(z, (h, c) if self.state_type == 'LSTM' else (h,None))\n",
        "\n",
        "            # Update hidden state based on GRU or LSTM\n",
        "            rnn_input = torch.cat([x_enc, z_enc], dim=1)\n",
        "            if self.state_type == 'LSTM':\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h,_ = self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "            # Calculate mutual information terms\n",
        "            neg_entropy += (-0.5 * self.encoder.encoder_net.out_features // 2 * math.log(2 * math.pi)\n",
        "                            - 0.5 * (1 + 2 * logsigma).sum(-1)).mean()\n",
        "\n",
        "            # Calculate log density for log_qz\n",
        "            var = logsigma.exp() ** 2\n",
        "            z = z.unsqueeze(1)\n",
        "            mu = mu.unsqueeze(0)\n",
        "            logsigma = logsigma.unsqueeze(0)\n",
        "            dev = z - mu\n",
        "            log_density = -0.5 * (dev ** 2 / var).sum(dim=-1) - 0.5 * (\n",
        "                self.encoder.encoder_net.out_features // 2 * math.log(2 * math.pi) + (2 * logsigma).sum(dim=-1))\n",
        "            log_qz1 = torch.logsumexp(log_density, dim=1) - math.log(batch_size)\n",
        "            log_qz += log_qz1.mean(-1)\n",
        "\n",
        "        # Calculate final MI value\n",
        "        mi = (neg_entropy / inputs.size(1)) - (log_qz / inputs.size(1))\n",
        "        return mi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0pVlwIe7V7D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class HybridStateUpdate(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(HybridStateUpdate, self).__init__()\n",
        "        self.gru = GRUState(latent_dim)\n",
        "        self.lstm = LSTMState(latent_dim)\n",
        "        self.alpha = nn.Parameter(torch.tensor(1.0))  # وزن المزيج بين GRU و LSTM\n",
        "\n",
        "    def forward(self, x_enc, z_enc, h, c=None):\n",
        "        h_gru, _ = self.gru(x_enc, z_enc, h)\n",
        "        h_lstm, c_new = self.lstm(x_enc, z_enc, h, c)\n",
        "        h_new = self.alpha * h_gru + (self.alpha) * h_lstm\n",
        "\n",
        "        if c is not None:\n",
        "            return h_new, c_new\n",
        "        else:\n",
        "            return h_new, None\n",
        "\n",
        "class VRNNAnomalyQuality(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, state_type=\"Hybrid\"):\n",
        "        super(VRNNAnomalyQuality, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(input_dim, latent_dim)\n",
        "        self.prior = Prior(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim, input_dim)\n",
        "\n",
        "        # Choose the state mechanism\n",
        "        if state_type == \"GRU\":\n",
        "            self.state_update = GRUState(latent_dim)\n",
        "        elif state_type == \"LSTM\":\n",
        "            self.state_update = LSTMState(latent_dim)\n",
        "        elif state_type == \"Hybrid\":\n",
        "            self.state_update = HybridStateUpdate(latent_dim)\n",
        "        else:\n",
        "            raise ValueError(\"state_type must be 'GRU', 'LSTM', or 'Hybrid'\")\n",
        "\n",
        "        self.state_type = state_type\n",
        "        self.h_0 = torch.zeros(1, latent_dim)\n",
        "        self.c_0 = torch.zeros(1, latent_dim) if state_type in [\"LSTM\", \"Hybrid\"] else None\n",
        "        self.threshold = 0.0001\n",
        "\n",
        "    def forward(self, x, beta=1.0):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h = self.h_0.expand(batch_size, -1).to(x.device)\n",
        "        c = self.c_0.expand(batch_size, -1).to(x.device) if self.c_0 is not None else None\n",
        "        listlogis = []\n",
        "\n",
        "        total_loss, kl_divergence, recon_loss = 0, 0, 0\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            prior_dist = self.prior((h, c) if self.c_0 is not None else (h, None))\n",
        "            posterior_dist, x_enc = self.encoder(x_t, (h, c) if self.c_0 is not None else (h, None))\n",
        "\n",
        "            z_t = posterior_dist.rsample()\n",
        "            x_recon, z_enc = self.decoder(z_t, (h, c) if self.c_0 is not None else (h, None))\n",
        "\n",
        "            if self.state_type == \"Hybrid\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            elif self.state_type == \"LSTM\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h, _ = self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "            kl_div = kl_divergence_diag_gaussians(posterior_dist, prior_dist).sum(dim=-1)\n",
        "            kl_divergence += kl_div.mean()\n",
        "            recon_loss += nn.functional.binary_cross_entropy(x_recon, x_t, reduction='sum')\n",
        "            listlogis.append(x_recon)\n",
        "\n",
        "        recon_loss /= seq_len*10\n",
        "        kl_divergence /= seq_len*10\n",
        "        total_loss = recon_loss + beta * kl_divergence\n",
        "        return total_loss, recon_loss, kl_divergence\n",
        "\n",
        "    def get_logis(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h = self.h_0.expand(batch_size, -1).to(x.device)\n",
        "        c = self.c_0.expand(batch_size, -1).to(x.device) if self.c_0 is not None else None\n",
        "        listlogis = []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            prior_dist = self.prior((h, c) if self.c_0 is not None else (h, None))\n",
        "            posterior_dist, x_enc = self.encoder(x_t, (h, c) if self.c_0 is not None else (h, None))\n",
        "            z_t = posterior_dist.rsample()\n",
        "            x_recon, z_enc = self.decoder(z_t, (h, c) if self.c_0 is not None else (h, None))\n",
        "            listlogis.append(x_recon)\n",
        "\n",
        "            if self.state_type == \"Hybrid\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            elif self.state_type == \"LSTM\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h, _ = self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "        x_rect = torch.stack(listlogis, dim=0)\n",
        "        x_rect = x_rect.permute(1, 0, 2)\n",
        "        return x_rect\n",
        "\n",
        "    def calculate_anomaly_rate(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        h = self.h_0.expand(batch_size, -1).contiguous().to(inputs.device)\n",
        "        c = self.c_0.expand(batch_size, -1).contiguous().to(inputs.device) if self.c_0 is not None else None\n",
        "\n",
        "        total_anomalies = 0\n",
        "        total_points = 0\n",
        "\n",
        "        for t in range(inputs.size(1)):\n",
        "            x = inputs[:, t, :]\n",
        "            posterior_dist, x_enc = self.encoder(x, (h, c) if self.c_0 is not None else (h, None))\n",
        "            z = posterior_dist.rsample()\n",
        "            x_recon, z_enc = self.decoder(z, (h, c) if self.c_0 is not None else (h, None))\n",
        "\n",
        "            diff = torch.abs(x - x_recon)\n",
        "            anomalies = (diff > self.threshold).sum(dim=-1)\n",
        "\n",
        "            total_anomalies += anomalies.sum().item()\n",
        "            total_points += x.numel()\n",
        "\n",
        "            if self.state_type == \"Hybrid\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            elif self.state_type == \"LSTM\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h, _ = self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "        anomaly_rate = total_anomalies / total_points\n",
        "        return anomaly_rate\n",
        "\n",
        "    def calc_mi(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        h = self.h_0.expand(batch_size, -1).contiguous().to(inputs.device)\n",
        "        c = self.c_0.expand(batch_size, -1).contiguous().to(inputs.device) if self.c_0 is not None else None\n",
        "\n",
        "        neg_entropy = 0\n",
        "        log_qz = 0\n",
        "\n",
        "        for t in range(inputs.size(1)):\n",
        "            x = inputs[:, t, :]\n",
        "            posterior_dist, x_enc = self.encoder(x, (h, c) if self.c_0 is not None else (h, None))\n",
        "            pz = self.prior((h, c) if self.c_0 is not None else h)\n",
        "\n",
        "            mu = posterior_dist.mu\n",
        "            logsigma = torch.log(posterior_dist.sigma)\n",
        "            z = posterior_dist.rsample()\n",
        "            _, z_enc = self.decoder(z, (h, c) if self.c_0 is not None else (h, None))\n",
        "\n",
        "            if self.state_type == \"Hybrid\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            elif self.state_type == \"LSTM\":\n",
        "                h, c = self.state_update(x_enc, z_enc, h, c)\n",
        "            else:\n",
        "                h, _ = self.state_update(x_enc, z_enc, h)\n",
        "\n",
        "            neg_entropy += (-0.5 * self.encoder.encoder_net.out_features // 2 * math.log(2 * math.pi)\n",
        "                            - 0.5 * (1 + 2 * logsigma).sum(-1)).mean()\n",
        "\n",
        "            var = logsigma.exp() ** 2\n",
        "            z = z.unsqueeze(1)\n",
        "            mu = mu.unsqueeze(0)\n",
        "            logsigma = logsigma.unsqueeze(0)\n",
        "            dev = z - mu\n",
        "            log_density = -0.5 * (dev ** 2 / var).sum(dim=-1) - 0.5 * (\n",
        "                self.encoder.encoder_net.out_features // 2 * math.log(2 * math.pi) + (2 * logsigma).sum(dim=-1))\n",
        "            log_qz1 = torch.logsumexp(log_density, dim=1) - math.log(batch_size)\n",
        "            log_qz += log_qz1.mean(-1)\n",
        "\n",
        "        mi = (neg_entropy / inputs.size(1)) - (log_qz / inputs.size(1))\n",
        "        return mi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zGm3PIF7d1y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW0HKbOL8nyu"
      },
      "outputs": [],
      "source": [
        "# @title VRNNVotingModel\n",
        "\n",
        "class VRNNVotingModel(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VRNNVotingModel, self).__init__()\n",
        "        self.vrnn_gru = VRNNAnomalyQuality(input_dim, latent_dim, state_type=\"GRU\")\n",
        "        self.vrnn_lstm = VRNNAnomalyQuality(input_dim, latent_dim, state_type=\"LSTM\")\n",
        "        self.state_type = \"Voting\"\n",
        "\n",
        "    def forward(self, x, beta=1.0):\n",
        "        # Train both models and get their losses\n",
        "        loss_gru, recon_loss_gru, kl_div_gru = self.vrnn_gru(x, beta)\n",
        "        loss_lstm, recon_loss_lstm, kl_div_lstm = self.vrnn_lstm(x, beta)\n",
        "\n",
        "        # Take the average of the losses as the final loss (voting mechanism)\n",
        "        total_loss = (loss_gru + loss_lstm) / 2\n",
        "        recon_loss = (recon_loss_gru + recon_loss_lstm) / 2\n",
        "        kl_divergence = (kl_div_gru + kl_div_lstm) / 2\n",
        "\n",
        "        return total_loss, recon_loss, kl_divergence\n",
        "\n",
        "    def calculate_anomaly_rate(self, inputs):\n",
        "        # Calculate anomaly rate for both GRU and LSTM models\n",
        "        anomaly_rate_gru = self.vrnn_gru.calculate_anomaly_rate(inputs)\n",
        "        anomaly_rate_lstm = self.vrnn_lstm.calculate_anomaly_rate(inputs)\n",
        "\n",
        "        # Take the average anomaly rate (voting mechanism)\n",
        "        anomaly_rate = (anomaly_rate_gru + anomaly_rate_lstm) / 2\n",
        "        return anomaly_rate\n",
        "\n",
        "    def calc_mi(self, inputs):\n",
        "        # Calculate mutual information for both GRU and LSTM models\n",
        "        mi_gru = self.vrnn_gru.calc_mi(inputs)\n",
        "        mi_lstm = self.vrnn_lstm.calc_mi(inputs)\n",
        "\n",
        "        # Take the average mutual information (voting mechanism)\n",
        "        mi = (mi_gru + mi_lstm) / 2\n",
        "        return mi\n",
        "\n",
        "    def get_logis(self, x):\n",
        "\n",
        "       logis_gru=self.vrnn_gru.get_logis(x)\n",
        "       logis_lstm=self.vrnn_lstm.get_logis(x)\n",
        "       logis=(logis_gru+logis_lstm)/2\n",
        "       return logis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVXVl4cu-l0l"
      },
      "outputs": [],
      "source": [
        "# @title Batch Dataset\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n",
        "def padded_sequences_batch(batch):\n",
        "\n",
        "    sequences = []\n",
        "\n",
        "    for sequence in batch:  # Iterate over each item in the batch\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    # Pad the sequences\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
        "    padded_sequences = padded_sequences.float()\n",
        "\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32# @param {type: \"number\"}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_size=0.7 #@param {type: \"number\"}\n",
        "train_size = int(train_size * len(dataset))\n",
        "val_size= 0.15 #@param {type: \"number\"}\n",
        "val_size = int(val_size * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,collate_fn=padded_sequences_batch)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,collate_fn=padded_sequences_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=padded_sequences_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH8p3Yg4CvAC",
        "outputId": "a4c9d356-b9f7-4294-ea28-8ec17a17e870"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqXv-2lO5WYs",
        "outputId": "9f3b1a8e-d1f7-4e14-c3dc-aceba224407b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmodelasg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import wandb\n",
        "# @title login  wandb\n",
        "\n",
        "key='782b6a6e82bbb5a5348de0d3c7d40d1e76351e79' # @param {type: \"string\"}\n",
        "wandb.login(key=key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOXwtgjJ5Sit"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QicbkrrK9ER8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2jbf1Ky8Lxy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2_dcSxcqYv-U"
      },
      "outputs": [],
      "source": [
        "# @title  train_model_wandb_long\n",
        "\n",
        "import wandb\n",
        "# تدريب مدى طويل  ملاحظة :\n",
        "def train_model_wandb_long(model,optimizer ,train_loader,test_loader, epochs=10,step_eval=10,name_project=\"anomaly-detection\"):\n",
        "\n",
        "    lst_lossall=[]\n",
        "    lst_klall=[]\n",
        "    lst_reconall=[]\n",
        "    lst_anomaly_rate=[]\n",
        "    lst_test_anomaly_rate=[]\n",
        "    lst_test_mi=[]\n",
        "    # model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, epoch_kl, epoch_recon = 0, 0, 0\n",
        "        anomaly_rate = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, x_batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = x_batch.to(device)\n",
        "            batch_data = torch.clamp(batch_data , 0.0,0.99)\n",
        "            total_loss, recon_loss, kl_divergence = model(batch_data)\n",
        "\n",
        "            anomaly_rate += model.calculate_anomaly_rate(batch_data)\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += total_loss.item()\n",
        "            epoch_recon += recon_loss.item()\n",
        "            epoch_kl += kl_divergence.item()\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"total_loss\": epoch_loss / len(train_loader),\n",
        "            \"reconstruction_loss\": epoch_recon / len(train_loader),\n",
        "            \"kl_divergence\": epoch_kl / len(train_loader),\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "        })\n",
        "        lst_lossall.append(epoch_loss/len(train_loader))\n",
        "        lst_klall.append(epoch_kl/len(train_loader))\n",
        "        lst_reconall.append(epoch_recon/len(train_loader))\n",
        "        lst_anomaly_rate.append(anomaly_rate/len(train_loader))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Recon Loss: {epoch_recon/len(train_loader):.4f}, KL: {epoch_kl/len(train_loader):.4f}, Anomaly Rate: {anomaly_rate/len(train_loader):.4f}\")\n",
        "        if (epoch+1) % step_eval == 0:\n",
        "            avg_anomaly_rate,mu_all, actual_trajectories, generated_trajectories = test_model_wandb(model, test_loader)\n",
        "\n",
        "            log_trajectories_to_wandb(actual_trajectories, generated_trajectories,num_samples=2)\n",
        "            lst_test_anomaly_rate.append(avg_anomaly_rate)\n",
        "            lst_test_mi.append(np.mean(mu_all))\n",
        "\n",
        "\n",
        "    return model,{f\"{model.state_type}\":{\"Loss\":lst_lossall,\n",
        "                                                 \"Recon-Loss\":lst_reconall,\n",
        "                                                 \"KL-Loss\":lst_klall,\n",
        "                                                 \"anomaly_rate\":lst_anomaly_rate,\n",
        "                                                 \"avg_anomaly_rate\":lst_test_anomaly_rate,\n",
        "                                                 \"avg_mi\":lst_test_mi,\n",
        "                                                 }}\n",
        "def test_model_wandb(model, test_loader):\n",
        "    model.eval()\n",
        "    anomaly_rates = []\n",
        "    mu_all=[]\n",
        "    actual_trajectories, generated_trajectories = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, x_batch in enumerate(test_loader):\n",
        "            batch_data =dataset[350+i].unsqueeze(0).to(device)\n",
        "            batch_data = torch.clamp(batch_data , 0.0, 0.99)\n",
        "\n",
        "\n",
        "\n",
        "            anomaly_rate = model.calculate_anomaly_rate(batch_data)\n",
        "            anomaly_rates.append(anomaly_rate)\n",
        "            mu_all.append(model.calc_mi(batch_data).cpu().numpy())\n",
        "\n",
        "            actual_trajectories.append(batch_data.cpu().numpy())\n",
        "            x_rect=model.get_logis(batch_data)\n",
        "\n",
        "\n",
        "            x_rect=x_rect.cpu().numpy()\n",
        "            generated_trajectories.append(x_rect)\n",
        "\n",
        "    avg_anomaly_rate = np.mean(anomaly_rates)\n",
        "    wandb.log({\"average_anomaly_rate_test\": avg_anomaly_rate})\n",
        "    print(f\"Average Anomaly Rate (Test): {avg_anomaly_rate:.4f}\")\n",
        "    wandb.log({\"average_mi_test\": np.mean(mu_all)})\n",
        "    print(f\"Average MI (Test): {np.mean(mu_all):.4f}\")\n",
        "    return avg_anomaly_rate,np.mean(mu_all), actual_trajectories, generated_trajectories\n",
        "\n",
        "# def log_trajectories_to_wandb(actual, generated, num_samples=5):\n",
        "\n",
        "\n",
        "#     for i in range(num_samples):\n",
        "#         data = {\n",
        "#             f\"Actual Trajectory {i+1}\": actual[i][0,:,0:2],\n",
        "#             f\"Generated Trajectory {i+1}\": generated[i][0,:,0:2],\n",
        "#         }\n",
        "#         wandb.log(data)\n",
        "def log_trajectories_to_wandb(actual, generated, num_samples=5):\n",
        "    num_samples = min(num_samples, len(actual))\n",
        "    for i in range(num_samples):\n",
        "        # Prepare data for line plot\n",
        "\n",
        "        actual_traj = actual[i][0]  # Extract X and Y for actual trajectory\n",
        "        generated_traj = generated[i][0]  # Extract X and Y for generated trajectory\n",
        "        actual_traj=dataset.dense_to_sparse(actual_traj)\n",
        "        generated_traj=dataset.dense_to_sparse(generated_traj)\n",
        "        actual_traj=actual_traj[:,0:2]\n",
        "        generated_traj=generated_traj[:,0:2]\n",
        "\n",
        "\n",
        "        # Convert to format suitable for W&B\n",
        "        table = wandb.Table(data=[\n",
        "            [int(x*LAT_BINS), int(y*LON_BINS), \"Actual\"] for x, y in actual_traj\n",
        "        ] + [\n",
        "            [int(x*LAT_BINS), int(y*LON_BINS), \"Generated\"] for x, y in generated_traj\n",
        "        ], columns=[\"x\", \"y\", \"Type\"])\n",
        "\n",
        "        # Log the line plot\n",
        "        wandb.log({f\"Trajectory {i+1}\": wandb.plot.line(\n",
        "            table, \"x\", \"y\", \"Type\", title=f\"Trajectory {i+1}\",\n",
        "\n",
        "\n",
        "\n",
        "        )})\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yDMZvqd-NisD"
      },
      "outputs": [],
      "source": [
        "# @title train_model_sciles\n",
        "\n",
        "import wandb\n",
        "\n",
        "# تدريب  شرائح\n",
        "def train_model_sciles(model,optimizer ,train_loader,test_loader, epochs=10,step_eval=10,name_project=\"anomaly-detection\"):\n",
        "\n",
        "    # model.train()\n",
        "    size_chack=10\n",
        "    lst_lossall=[]\n",
        "    lst_klall=[]\n",
        "    lst_reconall=[]\n",
        "    lst_anomaly_rate=[]\n",
        "    lst_test_anomaly_rate=[]\n",
        "    lst_test_mi=[]\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, epoch_kl, epoch_recon = 0, 0, 0\n",
        "        anomaly_rate = 0\n",
        "        model.train()\n",
        "\n",
        "\n",
        "        for i, x_batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = x_batch.to(device)\n",
        "            batch_data = torch.clamp(batch_data , 0.0, 0.99)\n",
        "\n",
        "            chacks=[ chack for chack in range(size_chack)]\n",
        "            sub_loss,sub_recon_loss, sub_kl_divergence = 0, 0, 0\n",
        "            for chack in chacks:\n",
        "                sub_batch=batch_data[:,chack:chack+size_chack,:]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                total_loss, recon_loss, kl_divergence = model(sub_batch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                total_loss.backward()\n",
        "\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                sub_loss += total_loss.item()\n",
        "                sub_recon_loss += recon_loss.item()\n",
        "                sub_kl_divergence += kl_divergence.item()\n",
        "\n",
        "\n",
        "            epoch_loss += sub_loss/len(chacks)\n",
        "            epoch_recon += sub_recon_loss/len(chacks)\n",
        "            epoch_kl += sub_kl_divergence/len(chacks)\n",
        "            anomaly_rate += model.calculate_anomaly_rate(batch_data)\n",
        "\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"total_loss\": epoch_loss / len(train_loader),\n",
        "            \"reconstruction_loss\": epoch_recon / len(train_loader),\n",
        "            \"kl_divergence\": epoch_kl / len(train_loader),\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "        })\n",
        "        lst_lossall.append(epoch_loss/len(train_loader))\n",
        "        lst_klall.append(epoch_kl/len(train_loader))\n",
        "        lst_reconall.append(epoch_recon/len(train_loader))\n",
        "        lst_anomaly_rate.append(anomaly_rate/len(train_loader))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Recon Loss: {epoch_recon/len(train_loader):.4f}, KL: {epoch_kl/len(train_loader):.4f}, Anomaly Rate: {anomaly_rate/len(train_loader):.4f}\")\n",
        "        if (epoch+1) % step_eval == 0:\n",
        "            avg_anomaly_rate,mu_all, actual_trajectories, generated_trajectories = test_model_wandb(model, test_loader)\n",
        "\n",
        "            log_trajectories_to_wandb(actual_trajectories, generated_trajectories,num_samples=2)\n",
        "            lst_test_anomaly_rate.append(avg_anomaly_rate)\n",
        "            lst_test_mi.append(np.mean(mu_all))\n",
        "\n",
        "\n",
        "    return model,{f\"{model.state_type}\":{\"Loss\":lst_lossall,\n",
        "                                                 \"Recon-Loss\":lst_reconall,\n",
        "                                                 \"KL-Loss\":lst_klall,\n",
        "                                                 \"anomaly_rate\":lst_anomaly_rate,\n",
        "                                                 \"avg_anomaly_rate\":lst_test_anomaly_rate,\n",
        "                                                 \"avg_mi\":lst_test_mi,\n",
        "                                                 }}\n",
        "def test_model_wandb(model, test_loader):\n",
        "    model.eval()\n",
        "    anomaly_rates = []\n",
        "    mu_all=[]\n",
        "    actual_trajectories, generated_trajectories = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, x_batch in enumerate(test_loader):\n",
        "            batch_data =dataset[350+i].unsqueeze(0).to(device)\n",
        "            batch_data = torch.clamp(batch_data , 0.0, 0.99)\n",
        "\n",
        "\n",
        "\n",
        "            anomaly_rate = model.calculate_anomaly_rate(batch_data)\n",
        "            anomaly_rates.append(anomaly_rate)\n",
        "            mu_all.append(model.calc_mi(batch_data).cpu().numpy())\n",
        "\n",
        "            actual_trajectories.append(batch_data.cpu().numpy())\n",
        "            x_rect=model.get_logis(batch_data)\n",
        "\n",
        "\n",
        "            x_rect=x_rect.cpu().numpy()\n",
        "            generated_trajectories.append(x_rect)\n",
        "\n",
        "    avg_anomaly_rate = np.mean(anomaly_rates)\n",
        "    wandb.log({\"average_anomaly_rate_test\": avg_anomaly_rate})\n",
        "    print(f\"Average Anomaly Rate (Test): {avg_anomaly_rate:.4f}\")\n",
        "    wandb.log({\"average_mi_test\": np.mean(mu_all)})\n",
        "    print(f\"Average MI (Test): {np.mean(mu_all):.4f}\")\n",
        "    return avg_anomaly_rate,np.mean(mu_all), actual_trajectories, generated_trajectories\n",
        "\n",
        "# def log_trajectories_to_wandb(actual, generated, num_samples=5):\n",
        "\n",
        "\n",
        "#     for i in range(num_samples):\n",
        "#         data = {\n",
        "#             f\"Actual Trajectory Four-hot  {i+1}\": actual[i][0,:,0:2],\n",
        "#             f\"Generated Trajectory Four-hot {i+1}\": generated[i][0,:,0:2],\n",
        "#         }\n",
        "#         wandb.log(data)\n",
        "def log_trajectories_to_wandb(actual, generated, num_samples=5):\n",
        "\n",
        "    num_samples = min(num_samples, len(actual))\n",
        "    for i in range(num_samples):\n",
        "        actual_traj = actual[i][0]\n",
        "        generated_traj = generated[i][0]\n",
        "        actual_traj=dataset.dense_to_sparse(actual_traj)\n",
        "        generated_traj=dataset.dense_to_sparse(generated_traj)\n",
        "        actual_traj=actual_traj[:,0:2]\n",
        "        generated_traj=generated_traj[:,0:2]\n",
        "        table = wandb.Table(data=[\n",
        "            [int(x*LAT_BINS), int(y*LON_BINS), \"Actual\"] for x, y in actual_traj\n",
        "        ] + [\n",
        "            [int(x*LAT_BINS), int(y*LON_BINS), \"Generated\"] for x, y in generated_traj\n",
        "        ], columns=[\"x\", \"y\", \"Type\"])\n",
        "\n",
        "        wandb.log({f\"Trajectory {i+1}\": wandb.plot.line(\n",
        "            table, \"x\", \"y\", \"Type\", title=f\"Trajectory {i+1}\",\n",
        "\n",
        "\n",
        "\n",
        "        )})\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RW-2zZJDYH_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cBSNRn-Y2O_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTTufsCA9F5p",
        "outputId": "e570dffc-2535-46ad-b69f-8721546772e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created model\n"
          ]
        }
      ],
      "source": [
        "# @title create model  and  optimizer\n",
        "\n",
        "input_dim=dataset.total_bins\n",
        "latent_dim=100 # @param {type: \"number\"}\n",
        "type_model='Hybrid' # @param ['Hybrid','GRU','LSTM','GRU_LSTM']\n",
        "# Initialize model\n",
        "if type_model=='GRU_LSTM':\n",
        "    model = VRNNVotingModel(input_dim=input_dim, latent_dim=latent_dim).to(device)\n",
        "else :\n",
        "    model = VRNNAnomalyQuality(input_dim=input_dim, latent_dim=latent_dim,state_type=type_model).to(device)\n",
        "\n",
        "# @markdown  optimizer\n",
        "lr=0.001 # @param {type: \"number\"}\n",
        "beta0=0.8 #@param {type: \"number\"}\n",
        "beta1=0.999 # @param {type: \"number\"}\n",
        "eps=1e-08 # @param {type: \"number\"}\n",
        "weight_decay=0.0 # @param {type: \"number\"}\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr,betas=(beta0,beta1), eps=1e-08, weight_decay=0)\n",
        "\n",
        "print('created model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RvTYZfDS0_9"
      },
      "outputs": [],
      "source": [
        "def  create_model(input_dim,latent_dim,type_model):\n",
        "    if type_model=='GRU_LSTM':\n",
        "        model = VRNNVotingModel(input_dim=input_dim, latent_dim=latent_dim).to(device)\n",
        "        return model\n",
        "    else :\n",
        "        model = VRNNAnomalyQuality(input_dim=input_dim, latent_dim=latent_dim,state_type=type_model).to(device)\n",
        "        return model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc5NPfjCY2SZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b050wSvcN-Mw",
        "outputId": "f516f3b3-e26a-439c-b3ad-86b1e979617b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 429.7744, Recon Loss: 406.3935, KL: 23.3809, Anomaly Rate: 1.0000\n",
            "Epoch 2/200, Loss: 117.6129, Recon Loss: 84.4217, KL: 33.1912, Anomaly Rate: 1.0000\n",
            "Epoch 3/200, Loss: 42.2382, Recon Loss: 32.3559, KL: 9.8824, Anomaly Rate: 1.0000\n",
            "Epoch 4/200, Loss: 31.9924, Recon Loss: 26.9041, KL: 5.0883, Anomaly Rate: 1.0000\n",
            "Epoch 5/200, Loss: 28.2342, Recon Loss: 24.8106, KL: 3.4235, Anomaly Rate: 0.9999\n",
            "Average Anomaly Rate (Test): 0.9997\n",
            "Average MI (Test): -0.2018\n",
            "Epoch 6/200, Loss: 26.2126, Recon Loss: 23.6671, KL: 2.5456, Anomaly Rate: 0.9996\n",
            "Epoch 7/200, Loss: 24.6336, Recon Loss: 22.6507, KL: 1.9828, Anomaly Rate: 0.9979\n",
            "Epoch 8/200, Loss: 24.5474, Recon Loss: 22.8404, KL: 1.7071, Anomaly Rate: 0.9927\n",
            "Epoch 9/200, Loss: 24.0772, Recon Loss: 22.6323, KL: 1.4449, Anomaly Rate: 0.9793\n",
            "Epoch 10/200, Loss: 22.4255, Recon Loss: 21.1513, KL: 1.2742, Anomaly Rate: 0.9430\n",
            "Average Anomaly Rate (Test): 0.9533\n",
            "Average MI (Test): -0.0224\n",
            "Epoch 11/200, Loss: 22.7365, Recon Loss: 21.2299, KL: 1.5066, Anomaly Rate: 0.9168\n",
            "Epoch 12/200, Loss: 21.1466, Recon Loss: 19.9933, KL: 1.1533, Anomaly Rate: 0.8429\n",
            "Epoch 13/200, Loss: 20.5911, Recon Loss: 19.0885, KL: 1.5026, Anomaly Rate: 0.7416\n",
            "Epoch 14/200, Loss: 19.7782, Recon Loss: 18.2705, KL: 1.5078, Anomaly Rate: 0.6619\n",
            "Epoch 15/200, Loss: 18.6374, Recon Loss: 17.1945, KL: 1.4429, Anomaly Rate: 0.5826\n",
            "Average Anomaly Rate (Test): 0.9388\n",
            "Average MI (Test): -0.3562\n",
            "Epoch 16/200, Loss: 18.0084, Recon Loss: 16.6814, KL: 1.3270, Anomaly Rate: 0.5473\n",
            "Epoch 17/200, Loss: 18.1331, Recon Loss: 16.9702, KL: 1.1629, Anomaly Rate: 0.5228\n",
            "Epoch 18/200, Loss: 17.7511, Recon Loss: 16.6876, KL: 1.0635, Anomaly Rate: 0.5052\n",
            "Epoch 19/200, Loss: 17.8290, Recon Loss: 16.8289, KL: 1.0001, Anomaly Rate: 0.4914\n",
            "Epoch 20/200, Loss: 17.2280, Recon Loss: 16.3123, KL: 0.9157, Anomaly Rate: 0.4628\n",
            "Average Anomaly Rate (Test): 0.7992\n",
            "Average MI (Test): -0.6893\n",
            "Epoch 21/200, Loss: 16.8681, Recon Loss: 16.0331, KL: 0.8350, Anomaly Rate: 0.4381\n",
            "Epoch 22/200, Loss: 17.0076, Recon Loss: 16.0939, KL: 0.9138, Anomaly Rate: 0.4213\n",
            "Epoch 23/200, Loss: 16.6056, Recon Loss: 15.8123, KL: 0.7933, Anomaly Rate: 0.4124\n",
            "Epoch 24/200, Loss: 16.7795, Recon Loss: 15.9686, KL: 0.8110, Anomaly Rate: 0.3982\n",
            "Epoch 25/200, Loss: 16.4616, Recon Loss: 15.7020, KL: 0.7596, Anomaly Rate: 0.3795\n",
            "Average Anomaly Rate (Test): 0.6165\n",
            "Average MI (Test): 0.4625\n",
            "Epoch 26/200, Loss: 16.6995, Recon Loss: 15.9228, KL: 0.7767, Anomaly Rate: 0.3671\n",
            "Epoch 27/200, Loss: 16.1952, Recon Loss: 15.4158, KL: 0.7794, Anomaly Rate: 0.3447\n",
            "Epoch 28/200, Loss: 16.1718, Recon Loss: 15.3578, KL: 0.8140, Anomaly Rate: 0.3447\n",
            "Epoch 29/200, Loss: 15.4496, Recon Loss: 14.5879, KL: 0.8617, Anomaly Rate: 0.3316\n",
            "Epoch 30/200, Loss: 15.3098, Recon Loss: 14.3273, KL: 0.9826, Anomaly Rate: 0.3279\n",
            "Average Anomaly Rate (Test): 0.5962\n",
            "Average MI (Test): -0.1341\n",
            "Epoch 31/200, Loss: 15.0776, Recon Loss: 14.3467, KL: 0.7309, Anomaly Rate: 0.3182\n",
            "Epoch 32/200, Loss: 14.5326, Recon Loss: 13.9027, KL: 0.6299, Anomaly Rate: 0.2985\n",
            "Epoch 33/200, Loss: 14.0876, Recon Loss: 13.4798, KL: 0.6078, Anomaly Rate: 0.2744\n",
            "Epoch 34/200, Loss: 14.2920, Recon Loss: 13.7046, KL: 0.5873, Anomaly Rate: 0.2708\n",
            "Epoch 35/200, Loss: 14.2123, Recon Loss: 13.6038, KL: 0.6084, Anomaly Rate: 0.2674\n",
            "Average Anomaly Rate (Test): 0.4482\n",
            "Average MI (Test): -0.0411\n",
            "Epoch 36/200, Loss: 14.2751, Recon Loss: 13.7108, KL: 0.5642, Anomaly Rate: 0.2646\n",
            "Epoch 37/200, Loss: 13.7529, Recon Loss: 13.2129, KL: 0.5399, Anomaly Rate: 0.2544\n",
            "Epoch 38/200, Loss: 13.8931, Recon Loss: 13.3254, KL: 0.5677, Anomaly Rate: 0.2446\n",
            "Epoch 39/200, Loss: 14.0967, Recon Loss: 13.5531, KL: 0.5436, Anomaly Rate: 0.2480\n",
            "Epoch 40/200, Loss: 13.6640, Recon Loss: 13.1594, KL: 0.5046, Anomaly Rate: 0.2425\n",
            "Average Anomaly Rate (Test): 0.3697\n",
            "Average MI (Test): -0.4623\n",
            "Epoch 41/200, Loss: 13.7930, Recon Loss: 13.2858, KL: 0.5072, Anomaly Rate: 0.2372\n",
            "Epoch 42/200, Loss: 13.6262, Recon Loss: 13.1306, KL: 0.4956, Anomaly Rate: 0.2333\n",
            "Epoch 43/200, Loss: 13.5704, Recon Loss: 13.0799, KL: 0.4905, Anomaly Rate: 0.2255\n",
            "Epoch 44/200, Loss: 13.4142, Recon Loss: 12.9276, KL: 0.4867, Anomaly Rate: 0.2225\n",
            "Epoch 45/200, Loss: 13.2666, Recon Loss: 12.7770, KL: 0.4897, Anomaly Rate: 0.2090\n",
            "Average Anomaly Rate (Test): 0.4258\n",
            "Average MI (Test): -1.0019\n",
            "Epoch 46/200, Loss: 13.3866, Recon Loss: 12.8394, KL: 0.5473, Anomaly Rate: 0.2107\n",
            "Epoch 47/200, Loss: 13.1798, Recon Loss: 12.6959, KL: 0.4839, Anomaly Rate: 0.2058\n",
            "Epoch 48/200, Loss: 13.2405, Recon Loss: 12.7327, KL: 0.5078, Anomaly Rate: 0.1949\n",
            "Epoch 49/200, Loss: 12.8537, Recon Loss: 12.3390, KL: 0.5147, Anomaly Rate: 0.1855\n",
            "Epoch 50/200, Loss: 13.1881, Recon Loss: 12.5755, KL: 0.6126, Anomaly Rate: 0.1895\n",
            "Average Anomaly Rate (Test): 0.3310\n",
            "Average MI (Test): -0.4728\n",
            "Epoch 51/200, Loss: 12.6940, Recon Loss: 12.1030, KL: 0.5910, Anomaly Rate: 0.1851\n",
            "Epoch 52/200, Loss: 12.8350, Recon Loss: 12.2238, KL: 0.6112, Anomaly Rate: 0.1826\n",
            "Epoch 53/200, Loss: 12.3977, Recon Loss: 11.8346, KL: 0.5630, Anomaly Rate: 0.1790\n",
            "Epoch 54/200, Loss: 12.2997, Recon Loss: 11.7507, KL: 0.5490, Anomaly Rate: 0.1797\n",
            "Epoch 55/200, Loss: 12.1051, Recon Loss: 11.6100, KL: 0.4952, Anomaly Rate: 0.1768\n",
            "Average Anomaly Rate (Test): 0.3217\n",
            "Average MI (Test): -1.0818\n",
            "Epoch 56/200, Loss: 12.2422, Recon Loss: 11.7612, KL: 0.4810, Anomaly Rate: 0.1747\n",
            "Epoch 57/200, Loss: 12.1825, Recon Loss: 11.6537, KL: 0.5289, Anomaly Rate: 0.1720\n",
            "Epoch 58/200, Loss: 11.7081, Recon Loss: 11.2246, KL: 0.4835, Anomaly Rate: 0.1656\n",
            "Epoch 59/200, Loss: 11.4570, Recon Loss: 10.9810, KL: 0.4760, Anomaly Rate: 0.1656\n"
          ]
        }
      ],
      "source": [
        "# @title Training And Testing\n",
        "\n",
        "name_project='anomaly-detection-listm_200' # @param {type: \"string\"}\n",
        "wandb.init(\n",
        "    project=name_project,\n",
        "    name=\"trajectory-model-listall\",\n",
        "    config={\n",
        "        \"epochs\": 10,\n",
        "        \"batch_size\": 32,\n",
        "        \"learning_rate\": 0.001,\n",
        "    },\n",
        "    )\n",
        "name_project_wandb='anomaly-detection-listm_200' # @param {type: \"string\"}\n",
        "epochs=200 # @param {type: \"number\"}\n",
        "step_eval=5 # @param {type: \"number\"}\n",
        "type_train='Long' # @param ['Long','sciles']\n",
        "if type_train=='Long':\n",
        "    outs=train_model_wandb_long(model,optimizer ,train_loader,test_loader, epochs=epochs,step_eval=step_eval,name_project=name_project_wandb)\n",
        "else :\n",
        "    outs=train_model_sciles(model,optimizer ,train_loader,test_loader, epochs=epochs,step_eval=step_eval,name_project=name_project_wandb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: outs save and download\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Save the model and training history\n",
        "model_filename = 'trained_model_long200.pth'\n",
        "history_filename = 'trained_model_long200.pkl'\n",
        "\n",
        "torch.save(outs[0].state_dict(), model_filename)\n",
        "with open(history_filename, 'wb') as f:\n",
        "    pickle.dump(outs[1], f)\n",
        "\n",
        "# Download the saved files\n",
        "from google.colab import files\n",
        "files.download(model_filename)\n",
        "files.download(history_filename)"
      ],
      "metadata": {
        "id": "2xLEIntnTXkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy7TUZyAKo5n"
      },
      "outputs": [],
      "source": [
        "def  train_model_thrd(inputs):\n",
        "\n",
        "     input_dim,latent_dim,type_model,train_loader,test_loader=inputs\n",
        "     model=create_model(input_dim,latent_dim,type_model)\n",
        "\n",
        "     name_project_wandb='anomaly-detection-listm_200' # @param {type: \"string\"}\n",
        "     epochs=20 # @param {type: \"number\"}\n",
        "     step_eval=5 # @param {type: \"number\"}\n",
        "     # @markdown  optimizer\n",
        "      # @markdown  optimizer\n",
        "     lr=0.001 # @param {type: \"number\"}\n",
        "     beta0=0.8 #@param {type: \"number\"}\n",
        "     beta1=0.999 # @param {type: \"number\"}\n",
        "     eps=1e-08 # @param {type: \"number\"}\n",
        "     weight_decay=0.0 # @param {type: \"number\"}\n",
        "     optimizer = torch.optim.AdamW(model.parameters(), lr=lr,betas=(beta0,beta1), eps=1e-08, weight_decay=0)\n",
        "     type_train='sciles' # @param ['Long','sciles']\n",
        "     if type_train=='Long':\n",
        "        outs=train_model_wandb_long(model,optimizer ,train_loader,test_loader, epochs=epochs,step_eval=step_eval,name_project=name_project_wandb)\n",
        "     else :\n",
        "        outs=train_model_sciles(model,optimizer ,train_loader,test_loader, epochs=epochs,step_eval=step_eval,name_project=name_project_wandb)\n",
        "     torch.save(outs[0].state_dict(), f'/content/drive/MyDrive/t/model_{type_model}.pth')\n",
        "     return outs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtgDS98qFRMY"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "#LSTM','GRU_LSTM'\n",
        "typemodels=['Hybrid']\n",
        "input_dim=dataset.total_bins\n",
        "latent_dim=100 # @param {type: \"number\"}\n",
        "max_=10\n",
        "name_project='anomaly-detection-listm_200' # @param {type: \"string\"}\n",
        "wandb.init(\n",
        "    project=name_project,\n",
        "    name=\"trajectory-model-listall\",\n",
        "    config={\n",
        "        \"epochs\": 10,\n",
        "        \"batch_size\": 32,\n",
        "        \"learning_rate\": 0.001,\n",
        "    },\n",
        "    )\n",
        "inputs=[]\n",
        "for type_model in typemodels:\n",
        "    inputs.append((input_dim,latent_dim,type_model,train_loader,test_loader))\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=16) as executor:\n",
        "    results = list(executor.map(train_model_thrd, inputs))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the biLSTM model\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, bidirectional=True):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Define the biLSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Define a fully connected layer for output\n",
        "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the biLSTM layer\n",
        "        lstm_out, _ = self.lstm(x)  # lstm_out shape: (batch_size, seq_length, hidden_size * 2 if bidirectional)\n",
        "\n",
        "        # Use the final hidden states (forward and backward)\n",
        "        if self.bidirectional:\n",
        "            out = torch.cat((lstm_out[:, -1, :self.hidden_size], lstm_out[:, 0, self.hidden_size:]), dim=1)\n",
        "        else:\n",
        "            out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters\n",
        "    input_size = 10   # Size of each input vector\n",
        "    hidden_size = 20  # Number of features in the hidden state\n",
        "    output_size = 2   # Number of output classes\n",
        "    seq_length = 5    # Length of the input sequences\n",
        "    batch_size = 3    # Number of sequences in a batch\n",
        "\n",
        "    # Create an instance of the biLSTM model\n",
        "    model = BiLSTMModel(input_size, hidden_size, output_size)\n",
        "\n",
        "    # Create some random input data\n",
        "    inputs = torch.randn(batch_size, seq_length, input_size)  # (batch_size, seq_length, input_size)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Print the output shape\n",
        "    print(\"Output shape:\", outputs.shape)  # Expected: (batch_size, output_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0R4kUGWQ1WD",
        "outputId": "fe576ee1-0e96-473b-9c16-596377595e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNjS_GE0Gcxv"
      },
      "outputs": [],
      "source": [
        "model,metrics=results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv_0xU5wG7qS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "K-fxgFfVi3UV",
        "outputId": "cc43b803-16c4-45cb-9357-3a1f2302dc9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Timestamp  Speed  Course  Latitude  Longitude  \\\n",
              "0        2024-07-01 00:17:08   0.00  215.00  21.47355   39.16373   \n",
              "1        2024-07-01 00:17:08   0.00  215.00  21.47355   39.16373   \n",
              "2        2024-07-01 00:17:08   0.00  215.00  21.47355   39.16373   \n",
              "3        2024-07-01 00:17:08   0.00  215.00  21.47355   39.16373   \n",
              "4        2024-07-01 00:17:08   0.00  215.00  21.47355   39.16373   \n",
              "...                      ...    ...     ...       ...        ...   \n",
              "140998  2024-11-22 23:59 UTC  14.81  337.01  30.11000   33.15000   \n",
              "140999  2024-11-22 23:59 UTC  14.80  337.00  29.56003   32.57593   \n",
              "141000  2024-11-22 23:59 UTC  11.43  312.02  28.53000   34.26000   \n",
              "141001  2024-11-22 23:59 UTC  12.12  144.02  26.59000   35.70000   \n",
              "141002  2024-11-22 23:59 UTC   9.05  321.04  28.27000   34.93000   \n",
              "\n",
              "                                                   Vessel  \n",
              "0                                           NewVessel_217  \n",
              "1                                            NewVessel_13  \n",
              "2       MarineTraffic_Vessel_positions_Export_2024-07-...  \n",
              "3                                           NewVessel_299  \n",
              "4                                           NewVessel_259  \n",
              "...                                                   ...  \n",
              "140998                        AQUA LADY_9288435_249807000  \n",
              "140999                         SCIROCCO_9407835_636014043  \n",
              "141000                     FORTUNE STAR_9436721_538009251  \n",
              "141001                            NAXOS_9336426_352002674  \n",
              "141002                      DURRAT BAHR AHMAR_0_403218910  \n",
              "\n",
              "[141003 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0032504d-1a3d-4abf-af02-f08b6d262148\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Speed</th>\n",
              "      <th>Course</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>Vessel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-07-01 00:17:08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>215.00</td>\n",
              "      <td>21.47355</td>\n",
              "      <td>39.16373</td>\n",
              "      <td>NewVessel_217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-07-01 00:17:08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>215.00</td>\n",
              "      <td>21.47355</td>\n",
              "      <td>39.16373</td>\n",
              "      <td>NewVessel_13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-07-01 00:17:08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>215.00</td>\n",
              "      <td>21.47355</td>\n",
              "      <td>39.16373</td>\n",
              "      <td>MarineTraffic_Vessel_positions_Export_2024-07-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-07-01 00:17:08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>215.00</td>\n",
              "      <td>21.47355</td>\n",
              "      <td>39.16373</td>\n",
              "      <td>NewVessel_299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-07-01 00:17:08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>215.00</td>\n",
              "      <td>21.47355</td>\n",
              "      <td>39.16373</td>\n",
              "      <td>NewVessel_259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140998</th>\n",
              "      <td>2024-11-22 23:59 UTC</td>\n",
              "      <td>14.81</td>\n",
              "      <td>337.01</td>\n",
              "      <td>30.11000</td>\n",
              "      <td>33.15000</td>\n",
              "      <td>AQUA LADY_9288435_249807000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140999</th>\n",
              "      <td>2024-11-22 23:59 UTC</td>\n",
              "      <td>14.80</td>\n",
              "      <td>337.00</td>\n",
              "      <td>29.56003</td>\n",
              "      <td>32.57593</td>\n",
              "      <td>SCIROCCO_9407835_636014043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141000</th>\n",
              "      <td>2024-11-22 23:59 UTC</td>\n",
              "      <td>11.43</td>\n",
              "      <td>312.02</td>\n",
              "      <td>28.53000</td>\n",
              "      <td>34.26000</td>\n",
              "      <td>FORTUNE STAR_9436721_538009251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141001</th>\n",
              "      <td>2024-11-22 23:59 UTC</td>\n",
              "      <td>12.12</td>\n",
              "      <td>144.02</td>\n",
              "      <td>26.59000</td>\n",
              "      <td>35.70000</td>\n",
              "      <td>NAXOS_9336426_352002674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141002</th>\n",
              "      <td>2024-11-22 23:59 UTC</td>\n",
              "      <td>9.05</td>\n",
              "      <td>321.04</td>\n",
              "      <td>28.27000</td>\n",
              "      <td>34.93000</td>\n",
              "      <td>DURRAT BAHR AHMAR_0_403218910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>141003 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0032504d-1a3d-4abf-af02-f08b6d262148')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0032504d-1a3d-4abf-af02-f08b6d262148 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0032504d-1a3d-4abf-af02-f08b6d262148');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b00fc0fb-12e9-477c-919c-38b7376c45c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b00fc0fb-12e9-477c-919c-38b7376c45c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b00fc0fb-12e9-477c-919c-38b7376c45c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f10499da-4177-40ae-994a-a29a380ec0b5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f10499da-4177-40ae-994a-a29a380ec0b5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# prompt: /content/df.csv read\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/DS-ALS-REDSEA-CLEANED.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBLUtkx2P4OA"
      },
      "outputs": [],
      "source": [
        "Timestamp=df['Timestamp']\n",
        "Speed=df['Speed']\n",
        "Course=df['Course']\n",
        "Latitude=df['Latitude']\n",
        "Longitude=df['Longitude']\n",
        "Vessel=df['Vessel']\n",
        "new_df=pd.DataFrame({'Timestamp':Timestamp,'Speed':Speed,'Course':Course,'Latitude':Latitude,'Longitude':Longitude,'Vessel':Vessel})\n",
        "\n",
        "#  REMOVE  VALUE masked     IN TABLE\n",
        "df_cleaned = new_df.dropna()\n",
        "\n",
        "df_cleaned = df_cleaned.replace('masked', pd.NA)\n",
        "df_cleaned = df_cleaned.dropna()\n",
        "df_cleaned['Latitude'] = df_cleaned['Latitude'].astype(float)\n",
        "df_cleaned['Longitude'] = df_cleaned['Longitude'].astype(float)\n",
        "df_cleaned['Speed'] = df_cleaned['Speed'].astype(float)\n",
        "df_cleaned['Course']=df_cleaned['Course'].astype(float)\n",
        "\n",
        "df_cleaned.sort_values(by=['Timestamp'], inplace=True)\n",
        "df_cleaned.reset_index(drop=True, inplace=True)\n",
        "\n",
        "import torch\n",
        "vs=df_cleaned['Vessel'].unique()\n",
        "def  get_labels(num):\n",
        "     if num <100:\n",
        "       return 0\n",
        "     elif num <200:\n",
        "       return 1\n",
        "     else:\n",
        "       return 2\n",
        "data=[]\n",
        "labels=[]\n",
        "features=['Latitude','Longitude','Speed','Course']\n",
        "for v in vs:\n",
        "  v1=df_cleaned[df_cleaned['Vessel']==v]\n",
        "\n",
        "  data.append(torch.tensor(v1[features].values)/360)\n",
        "  labels.append([get_labels(len(v1))]*4)\n",
        "data=data*1\n",
        "labels=labels*1\n",
        "labels=torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9wyxSGXQfJm",
        "outputId": "6b18acef-19dd-4c04-9f46-eab8e35ff8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing route: 1/800\n",
            "Processing route: 2/800\n",
            "Processing route: 3/800\n",
            "Processing route: 4/800\n",
            "Processing route: 5/800\n",
            "Processing route: 6/800\n",
            "Processing route: 7/800\n",
            "Processing route: 8/800\n",
            "Processing route: 9/800\n",
            "Processing route: 10/800\n",
            "Processing route: 11/800\n",
            "Processing route: 12/800\n",
            "Processing route: 13/800\n",
            "Processing route: 14/800\n",
            "Processing route: 15/800\n",
            "Processing route: 16/800\n",
            "Processing route: 17/800\n",
            "Processing route: 18/800\n",
            "Processing route: 19/800\n",
            "Processing route: 20/800\n",
            "Processing route: 21/800\n",
            "Processing route: 22/800\n",
            "Processing route: 23/800\n",
            "Processing route: 24/800\n",
            "Processing route: 25/800\n",
            "Processing route: 26/800\n",
            "Processing route: 27/800\n",
            "Processing route: 28/800\n",
            "Processing route: 29/800\n",
            "Processing route: 30/800\n",
            "Processing route: 31/800\n",
            "Processing route: 32/800\n",
            "Processing route: 33/800\n",
            "Processing route: 34/800\n",
            "Processing route: 35/800\n",
            "Processing route: 36/800\n",
            "Processing route: 37/800\n",
            "Processing route: 38/800\n",
            "Processing route: 39/800\n",
            "Processing route: 40/800\n",
            "Processing route: 41/800\n",
            "Processing route: 42/800\n",
            "Processing route: 43/800\n",
            "Processing route: 44/800\n",
            "Processing route: 45/800\n",
            "Processing route: 46/800\n",
            "Processing route: 47/800\n",
            "Processing route: 48/800\n",
            "Processing route: 49/800\n",
            "Processing route: 50/800\n",
            "Processing route: 51/800\n",
            "Processing route: 52/800\n",
            "Processing route: 53/800\n",
            "Processing route: 54/800\n",
            "Processing route: 55/800\n",
            "Processing route: 56/800\n",
            "Processing route: 57/800\n",
            "Processing route: 58/800\n",
            "Processing route: 59/800\n",
            "Processing route: 60/800\n",
            "Processing route: 61/800\n",
            "Processing route: 62/800\n",
            "Processing route: 63/800\n",
            "Processing route: 64/800\n",
            "Processing route: 65/800\n",
            "Processing route: 66/800\n",
            "Processing route: 67/800\n",
            "Processing route: 68/800\n",
            "Processing route: 69/800\n",
            "Processing route: 70/800\n",
            "Processing route: 71/800\n",
            "Processing route: 72/800\n",
            "Processing route: 73/800\n",
            "Processing route: 74/800\n",
            "Processing route: 75/800\n",
            "Processing route: 76/800\n",
            "Processing route: 77/800\n",
            "Processing route: 78/800\n",
            "Processing route: 79/800\n",
            "Processing route: 80/800\n",
            "Processing route: 81/800\n",
            "Processing route: 82/800\n",
            "Processing route: 83/800\n",
            "Processing route: 84/800\n",
            "Processing route: 85/800\n",
            "Processing route: 86/800\n",
            "Processing route: 87/800\n",
            "Processing route: 88/800\n",
            "Processing route: 89/800\n",
            "Processing route: 90/800\n",
            "Processing route: 91/800\n",
            "Processing route: 92/800\n",
            "Processing route: 93/800\n",
            "Processing route: 94/800\n",
            "Processing route: 95/800\n",
            "Processing route: 96/800\n",
            "Processing route: 97/800\n",
            "Processing route: 98/800\n",
            "Processing route: 99/800\n",
            "Processing route: 100/800\n",
            "Processing route: 101/800\n",
            "Processing route: 102/800\n",
            "Processing route: 103/800\n",
            "Processing route: 104/800\n",
            "Processing route: 105/800\n",
            "Processing route: 106/800\n",
            "Processing route: 107/800\n",
            "Processing route: 108/800\n",
            "Processing route: 109/800\n",
            "Processing route: 110/800\n",
            "Processing route: 111/800\n",
            "Processing route: 112/800\n",
            "Processing route: 113/800\n",
            "Processing route: 114/800\n",
            "Processing route: 115/800\n",
            "Processing route: 116/800\n",
            "Processing route: 117/800\n",
            "Processing route: 118/800\n",
            "Processing route: 119/800\n",
            "Processing route: 120/800\n",
            "Processing route: 121/800\n",
            "Processing route: 122/800\n",
            "Processing route: 123/800\n",
            "Processing route: 124/800\n",
            "Processing route: 125/800\n",
            "Processing route: 126/800\n",
            "Processing route: 127/800\n",
            "Processing route: 128/800\n",
            "Processing route: 129/800\n",
            "Processing route: 130/800\n",
            "Processing route: 131/800\n",
            "Processing route: 132/800\n",
            "Processing route: 133/800\n",
            "Processing route: 134/800\n",
            "Processing route: 135/800\n",
            "Processing route: 136/800\n",
            "Processing route: 137/800\n",
            "Processing route: 138/800\n",
            "Processing route: 139/800\n",
            "Processing route: 140/800\n",
            "Processing route: 141/800\n",
            "Processing route: 142/800\n",
            "Processing route: 143/800\n",
            "Processing route: 144/800\n",
            "Processing route: 145/800\n",
            "Processing route: 146/800\n",
            "Processing route: 147/800\n",
            "Processing route: 148/800\n",
            "Processing route: 149/800\n",
            "Processing route: 150/800\n",
            "Processing route: 151/800\n",
            "Processing route: 152/800\n",
            "Processing route: 153/800\n",
            "Processing route: 154/800\n",
            "Processing route: 155/800\n",
            "Processing route: 156/800\n",
            "Processing route: 157/800\n",
            "Processing route: 158/800\n",
            "Processing route: 159/800\n",
            "Processing route: 160/800\n",
            "Processing route: 161/800\n",
            "Processing route: 162/800\n",
            "Processing route: 163/800\n",
            "Processing route: 164/800\n",
            "Processing route: 165/800\n",
            "Processing route: 166/800\n",
            "Processing route: 167/800\n",
            "Processing route: 168/800\n",
            "Processing route: 169/800\n",
            "Processing route: 170/800\n",
            "Processing route: 171/800\n",
            "Processing route: 172/800\n",
            "Processing route: 173/800\n",
            "Processing route: 174/800\n",
            "Processing route: 175/800\n",
            "Processing route: 176/800\n",
            "Processing route: 177/800\n",
            "Processing route: 178/800\n",
            "Processing route: 179/800\n",
            "Processing route: 180/800\n",
            "Processing route: 181/800\n",
            "Processing route: 182/800\n",
            "Processing route: 183/800\n",
            "Processing route: 184/800\n",
            "Processing route: 185/800\n",
            "Processing route: 186/800\n",
            "Processing route: 187/800\n",
            "Processing route: 188/800\n",
            "Processing route: 189/800\n",
            "Processing route: 190/800\n",
            "Processing route: 191/800\n",
            "Processing route: 192/800\n",
            "Processing route: 193/800\n",
            "Processing route: 194/800\n",
            "Processing route: 195/800\n",
            "Processing route: 196/800\n",
            "Processing route: 197/800\n",
            "Processing route: 198/800\n",
            "Processing route: 199/800\n",
            "Processing route: 200/800\n",
            "Processing route: 201/800\n",
            "Processing route: 202/800\n",
            "Processing route: 203/800\n",
            "Processing route: 204/800\n",
            "Processing route: 205/800\n",
            "Processing route: 206/800\n",
            "Processing route: 207/800\n",
            "Processing route: 208/800\n",
            "Processing route: 209/800\n",
            "Processing route: 210/800\n",
            "Processing route: 211/800\n",
            "Processing route: 212/800\n",
            "Processing route: 213/800\n",
            "Processing route: 214/800\n",
            "Processing route: 215/800\n",
            "Processing route: 216/800\n",
            "Processing route: 217/800\n",
            "Processing route: 218/800\n",
            "Processing route: 219/800\n",
            "Processing route: 220/800\n",
            "Processing route: 221/800\n",
            "Processing route: 222/800\n",
            "Processing route: 223/800\n",
            "Processing route: 224/800\n",
            "Processing route: 225/800\n",
            "Processing route: 226/800\n",
            "Processing route: 227/800\n",
            "Processing route: 228/800\n",
            "Processing route: 229/800\n",
            "Processing route: 230/800\n",
            "Processing route: 231/800\n",
            "Processing route: 232/800\n",
            "Processing route: 233/800\n",
            "Processing route: 234/800\n",
            "Processing route: 235/800\n",
            "Processing route: 236/800\n",
            "Processing route: 237/800\n",
            "Processing route: 238/800\n",
            "Processing route: 239/800\n",
            "Processing route: 240/800\n",
            "Processing route: 241/800\n",
            "Processing route: 242/800\n",
            "Processing route: 243/800\n",
            "Processing route: 244/800\n",
            "Processing route: 245/800\n",
            "Processing route: 246/800\n",
            "Processing route: 247/800\n",
            "Processing route: 248/800\n",
            "Processing route: 249/800\n",
            "Processing route: 250/800\n",
            "Processing route: 251/800\n",
            "Processing route: 252/800\n",
            "Processing route: 253/800\n",
            "Processing route: 254/800\n",
            "Processing route: 255/800\n",
            "Processing route: 256/800\n",
            "Processing route: 257/800\n",
            "Processing route: 258/800\n",
            "Processing route: 259/800\n",
            "Processing route: 260/800\n",
            "Processing route: 261/800\n",
            "Processing route: 262/800\n",
            "Processing route: 263/800\n",
            "Processing route: 264/800\n",
            "Processing route: 265/800\n",
            "Processing route: 266/800\n",
            "Processing route: 267/800\n",
            "Processing route: 268/800\n",
            "Processing route: 269/800\n",
            "Processing route: 270/800\n",
            "Processing route: 271/800\n",
            "Processing route: 272/800\n",
            "Processing route: 273/800\n",
            "Processing route: 274/800\n",
            "Processing route: 275/800\n",
            "Processing route: 276/800\n",
            "Processing route: 277/800\n",
            "Processing route: 278/800\n",
            "Processing route: 279/800\n",
            "Processing route: 280/800\n",
            "Processing route: 281/800\n",
            "Processing route: 282/800\n",
            "Processing route: 283/800\n",
            "Processing route: 284/800\n",
            "Processing route: 285/800\n",
            "Processing route: 286/800\n",
            "Processing route: 287/800\n",
            "Processing route: 288/800\n",
            "Processing route: 289/800\n",
            "Processing route: 290/800\n",
            "Processing route: 291/800\n",
            "Processing route: 292/800\n",
            "Processing route: 293/800\n",
            "Processing route: 294/800\n",
            "Processing route: 295/800\n",
            "Processing route: 296/800\n",
            "Processing route: 297/800\n",
            "Processing route: 298/800\n",
            "Processing route: 299/800\n",
            "Processing route: 300/800\n",
            "Processing route: 301/800\n",
            "Processing route: 302/800\n",
            "Processing route: 303/800\n",
            "Processing route: 304/800\n",
            "Processing route: 305/800\n",
            "Processing route: 306/800\n",
            "Processing route: 307/800\n",
            "Processing route: 308/800\n",
            "Processing route: 309/800\n",
            "Processing route: 310/800\n",
            "Processing route: 311/800\n",
            "Processing route: 312/800\n",
            "Processing route: 313/800\n",
            "Processing route: 314/800\n",
            "Processing route: 315/800\n",
            "Processing route: 316/800\n",
            "Processing route: 317/800\n",
            "Processing route: 318/800\n",
            "Processing route: 319/800\n",
            "Processing route: 320/800\n",
            "Processing route: 321/800\n",
            "Processing route: 322/800\n",
            "Processing route: 323/800\n",
            "Processing route: 324/800\n",
            "Processing route: 325/800\n",
            "Processing route: 326/800\n",
            "Processing route: 327/800\n",
            "Processing route: 328/800\n",
            "Processing route: 329/800\n",
            "Processing route: 330/800\n",
            "Processing route: 331/800\n",
            "Processing route: 332/800\n",
            "Processing route: 333/800\n",
            "Processing route: 334/800\n",
            "Processing route: 335/800\n",
            "Processing route: 336/800\n",
            "Processing route: 337/800\n",
            "Processing route: 338/800\n",
            "Processing route: 339/800\n",
            "Processing route: 340/800\n",
            "Processing route: 341/800\n",
            "Processing route: 342/800\n",
            "Processing route: 343/800\n",
            "Processing route: 344/800\n",
            "Processing route: 345/800\n",
            "Processing route: 346/800\n",
            "Processing route: 347/800\n",
            "Processing route: 348/800\n",
            "Processing route: 349/800\n",
            "Processing route: 350/800\n",
            "Processing route: 351/800\n",
            "Processing route: 352/800\n",
            "Processing route: 353/800\n",
            "Processing route: 354/800\n",
            "Processing route: 355/800\n",
            "Processing route: 356/800\n",
            "Processing route: 357/800\n",
            "Processing route: 358/800\n",
            "Processing route: 359/800\n",
            "Processing route: 360/800\n",
            "Processing route: 361/800\n",
            "Processing route: 362/800\n",
            "Processing route: 363/800\n",
            "Processing route: 364/800\n",
            "Processing route: 365/800\n",
            "Processing route: 366/800\n",
            "Processing route: 367/800\n",
            "Processing route: 368/800\n",
            "Processing route: 369/800\n",
            "Processing route: 370/800\n",
            "Processing route: 371/800\n",
            "Processing route: 372/800\n",
            "Processing route: 373/800\n",
            "Processing route: 374/800\n",
            "Processing route: 375/800\n",
            "Processing route: 376/800\n",
            "Processing route: 377/800\n",
            "Processing route: 378/800\n",
            "Processing route: 379/800\n",
            "Processing route: 380/800\n",
            "Processing route: 381/800\n",
            "Processing route: 382/800\n",
            "Processing route: 383/800\n",
            "Processing route: 384/800\n",
            "Processing route: 385/800\n",
            "Processing route: 386/800\n",
            "Processing route: 387/800\n",
            "Processing route: 388/800\n",
            "Processing route: 389/800\n",
            "Processing route: 390/800\n",
            "Processing route: 391/800\n",
            "Processing route: 392/800\n",
            "Processing route: 393/800\n",
            "Processing route: 394/800\n",
            "Processing route: 395/800\n",
            "Processing route: 396/800\n",
            "Processing route: 397/800\n",
            "Processing route: 398/800\n",
            "Processing route: 399/800\n",
            "Processing route: 400/800\n",
            "Processing route: 401/800\n",
            "Processing route: 402/800\n",
            "Processing route: 403/800\n",
            "Processing route: 404/800\n",
            "Processing route: 405/800\n",
            "Processing route: 406/800\n",
            "Processing route: 407/800\n",
            "Processing route: 408/800\n",
            "Processing route: 409/800\n",
            "Processing route: 410/800\n",
            "Processing route: 411/800\n",
            "Processing route: 412/800\n",
            "Processing route: 413/800\n",
            "Processing route: 414/800\n",
            "Processing route: 415/800\n",
            "Processing route: 416/800\n",
            "Processing route: 417/800\n",
            "Processing route: 418/800\n",
            "Processing route: 419/800\n",
            "Processing route: 420/800\n",
            "Processing route: 421/800\n",
            "Processing route: 422/800\n",
            "Processing route: 423/800\n",
            "Processing route: 424/800\n",
            "Processing route: 425/800\n",
            "Processing route: 426/800\n",
            "Processing route: 427/800\n",
            "Processing route: 428/800\n",
            "Processing route: 429/800\n",
            "Processing route: 430/800\n",
            "Processing route: 431/800\n",
            "Processing route: 432/800\n",
            "Processing route: 433/800\n",
            "Processing route: 434/800\n",
            "Processing route: 435/800\n",
            "Processing route: 436/800\n",
            "Processing route: 437/800\n",
            "Processing route: 438/800\n",
            "Processing route: 439/800\n",
            "Processing route: 440/800\n",
            "Processing route: 441/800\n",
            "Processing route: 442/800\n",
            "Processing route: 443/800\n",
            "Processing route: 444/800\n",
            "Processing route: 445/800\n",
            "Processing route: 446/800\n",
            "Processing route: 447/800\n",
            "Processing route: 448/800\n",
            "Processing route: 449/800\n",
            "Processing route: 450/800\n",
            "Processing route: 451/800\n",
            "Processing route: 452/800\n",
            "Processing route: 453/800\n",
            "Processing route: 454/800\n",
            "Processing route: 455/800\n",
            "Processing route: 456/800\n",
            "Processing route: 457/800\n",
            "Processing route: 458/800\n",
            "Processing route: 459/800\n",
            "Processing route: 460/800\n",
            "Processing route: 461/800\n",
            "Processing route: 462/800\n",
            "Processing route: 463/800\n",
            "Processing route: 464/800\n",
            "Processing route: 465/800\n",
            "Processing route: 466/800\n",
            "Processing route: 467/800\n",
            "Processing route: 468/800\n",
            "Processing route: 469/800\n",
            "Processing route: 470/800\n",
            "Processing route: 471/800\n",
            "Processing route: 472/800\n",
            "Processing route: 473/800\n",
            "Processing route: 474/800\n",
            "Processing route: 475/800\n",
            "Processing route: 476/800\n",
            "Processing route: 477/800\n",
            "Processing route: 478/800\n",
            "Processing route: 479/800\n",
            "Processing route: 480/800\n",
            "Processing route: 481/800\n",
            "Processing route: 482/800\n",
            "Processing route: 483/800\n",
            "Processing route: 484/800\n",
            "Processing route: 485/800\n",
            "Processing route: 486/800\n",
            "Processing route: 487/800\n",
            "Processing route: 488/800\n",
            "Processing route: 489/800\n",
            "Processing route: 490/800\n",
            "Processing route: 491/800\n",
            "Processing route: 492/800\n",
            "Processing route: 493/800\n",
            "Processing route: 494/800\n",
            "Processing route: 495/800\n",
            "Processing route: 496/800\n",
            "Processing route: 497/800\n",
            "Processing route: 498/800\n",
            "Processing route: 499/800\n",
            "Processing route: 500/800\n",
            "Processing route: 501/800\n",
            "Processing route: 502/800\n",
            "Processing route: 503/800\n",
            "Processing route: 504/800\n",
            "Processing route: 505/800\n",
            "Processing route: 506/800\n",
            "Processing route: 507/800\n",
            "Processing route: 508/800\n",
            "Processing route: 509/800\n",
            "Processing route: 510/800\n",
            "Processing route: 511/800\n",
            "Processing route: 512/800\n",
            "Processing route: 513/800\n",
            "Processing route: 514/800\n",
            "Processing route: 515/800\n",
            "Processing route: 516/800\n",
            "Processing route: 517/800\n",
            "Processing route: 518/800\n",
            "Processing route: 519/800\n",
            "Processing route: 520/800\n",
            "Processing route: 521/800\n",
            "Processing route: 522/800\n",
            "Processing route: 523/800\n",
            "Processing route: 524/800\n",
            "Processing route: 525/800\n",
            "Processing route: 526/800\n",
            "Processing route: 527/800\n",
            "Processing route: 528/800\n",
            "Processing route: 529/800\n",
            "Processing route: 530/800\n",
            "Processing route: 531/800\n",
            "Processing route: 532/800\n",
            "Processing route: 533/800\n",
            "Processing route: 534/800\n",
            "Processing route: 535/800\n",
            "Processing route: 536/800\n",
            "Processing route: 537/800\n",
            "Processing route: 538/800\n",
            "Processing route: 539/800\n",
            "Processing route: 540/800\n",
            "Processing route: 541/800\n",
            "Processing route: 542/800\n",
            "Processing route: 543/800\n",
            "Processing route: 544/800\n",
            "Processing route: 545/800\n",
            "Processing route: 546/800\n",
            "Processing route: 547/800\n",
            "Processing route: 548/800\n",
            "Processing route: 549/800\n",
            "Processing route: 550/800\n",
            "Processing route: 551/800\n",
            "Processing route: 552/800\n",
            "Processing route: 553/800\n",
            "Processing route: 554/800\n",
            "Processing route: 555/800\n",
            "Processing route: 556/800\n",
            "Processing route: 557/800\n",
            "Processing route: 558/800\n",
            "Processing route: 559/800\n",
            "Processing route: 560/800\n",
            "Processing route: 561/800\n",
            "Processing route: 562/800\n",
            "Processing route: 563/800\n",
            "Processing route: 564/800\n",
            "Processing route: 565/800\n",
            "Processing route: 566/800\n",
            "Processing route: 567/800\n",
            "Processing route: 568/800\n",
            "Processing route: 569/800\n",
            "Processing route: 570/800\n",
            "Processing route: 571/800\n",
            "Processing route: 572/800\n",
            "Processing route: 573/800\n",
            "Processing route: 574/800\n",
            "Processing route: 575/800\n",
            "Processing route: 576/800\n",
            "Processing route: 577/800\n",
            "Processing route: 578/800\n",
            "Processing route: 579/800\n",
            "Processing route: 580/800\n",
            "Processing route: 581/800\n",
            "Processing route: 582/800\n",
            "Processing route: 583/800\n",
            "Processing route: 584/800\n",
            "Processing route: 585/800\n",
            "Processing route: 586/800\n",
            "Processing route: 587/800\n",
            "Processing route: 588/800\n",
            "Processing route: 589/800\n",
            "Processing route: 590/800\n",
            "Processing route: 591/800\n",
            "Processing route: 592/800\n",
            "Processing route: 593/800\n",
            "Processing route: 594/800\n",
            "Processing route: 595/800\n",
            "Processing route: 596/800\n",
            "Processing route: 597/800\n",
            "Processing route: 598/800\n",
            "Processing route: 599/800\n",
            "Processing route: 600/800\n",
            "Processing route: 601/800\n",
            "Processing route: 602/800\n",
            "Processing route: 603/800\n",
            "Processing route: 604/800\n",
            "Processing route: 605/800\n",
            "Processing route: 606/800\n",
            "Processing route: 607/800\n",
            "Processing route: 608/800\n",
            "Processing route: 609/800\n",
            "Processing route: 610/800\n",
            "Processing route: 611/800\n",
            "Processing route: 612/800\n",
            "Processing route: 613/800\n",
            "Processing route: 614/800\n",
            "Processing route: 615/800\n",
            "Processing route: 616/800\n",
            "Processing route: 617/800\n",
            "Processing route: 618/800\n",
            "Processing route: 619/800\n",
            "Processing route: 620/800\n",
            "Processing route: 621/800\n",
            "Processing route: 622/800\n",
            "Processing route: 623/800\n",
            "Processing route: 624/800\n",
            "Processing route: 625/800\n",
            "Processing route: 626/800\n",
            "Processing route: 627/800\n",
            "Processing route: 628/800\n",
            "Processing route: 629/800\n",
            "Processing route: 630/800\n",
            "Processing route: 631/800\n",
            "Processing route: 632/800\n",
            "Processing route: 633/800\n",
            "Processing route: 634/800\n",
            "Processing route: 635/800\n",
            "Processing route: 636/800\n",
            "Processing route: 637/800\n",
            "Processing route: 638/800\n",
            "Processing route: 639/800\n",
            "Processing route: 640/800\n",
            "Processing route: 641/800\n",
            "Processing route: 642/800\n",
            "Processing route: 643/800\n",
            "Processing route: 644/800\n",
            "Processing route: 645/800\n",
            "Processing route: 646/800\n",
            "Processing route: 647/800\n",
            "Processing route: 648/800\n",
            "Processing route: 649/800\n",
            "Processing route: 650/800\n",
            "Processing route: 651/800\n",
            "Processing route: 652/800\n",
            "Processing route: 653/800\n",
            "Processing route: 654/800\n",
            "Processing route: 655/800\n",
            "Processing route: 656/800\n",
            "Processing route: 657/800\n",
            "Processing route: 658/800\n",
            "Processing route: 659/800\n",
            "Processing route: 660/800\n",
            "Processing route: 661/800\n",
            "Processing route: 662/800\n",
            "Processing route: 663/800\n",
            "Processing route: 664/800\n",
            "Processing route: 665/800\n",
            "Processing route: 666/800\n",
            "Processing route: 667/800\n",
            "Processing route: 668/800\n",
            "Processing route: 669/800\n",
            "Processing route: 670/800\n",
            "Processing route: 671/800\n",
            "Processing route: 672/800\n",
            "Processing route: 673/800\n",
            "Processing route: 674/800\n",
            "Processing route: 675/800\n",
            "Processing route: 676/800\n",
            "Processing route: 677/800\n",
            "Processing route: 678/800\n",
            "Processing route: 679/800\n",
            "Processing route: 680/800\n",
            "Processing route: 681/800\n",
            "Processing route: 682/800\n",
            "Processing route: 683/800\n",
            "Processing route: 684/800\n",
            "Processing route: 685/800\n",
            "Processing route: 686/800\n",
            "Processing route: 687/800\n",
            "Processing route: 688/800\n",
            "Processing route: 689/800\n",
            "Processing route: 690/800\n",
            "Processing route: 691/800\n",
            "Processing route: 692/800\n",
            "Processing route: 693/800\n",
            "Processing route: 694/800\n",
            "Processing route: 695/800\n",
            "Processing route: 696/800\n",
            "Processing route: 697/800\n",
            "Processing route: 698/800\n",
            "Processing route: 699/800\n",
            "Processing route: 700/800\n",
            "Processing route: 701/800\n",
            "Processing route: 702/800\n",
            "Processing route: 703/800\n",
            "Processing route: 704/800\n",
            "Processing route: 705/800\n",
            "Processing route: 706/800\n",
            "Processing route: 707/800\n",
            "Processing route: 708/800\n",
            "Processing route: 709/800\n",
            "Processing route: 710/800\n",
            "Processing route: 711/800\n",
            "Processing route: 712/800\n",
            "Processing route: 713/800\n",
            "Processing route: 714/800\n",
            "Processing route: 715/800\n",
            "Processing route: 716/800\n",
            "Processing route: 717/800\n",
            "Processing route: 718/800\n",
            "Processing route: 719/800\n",
            "Processing route: 720/800\n",
            "Processing route: 721/800\n",
            "Processing route: 722/800\n",
            "Processing route: 723/800\n",
            "Processing route: 724/800\n",
            "Processing route: 725/800\n",
            "Processing route: 726/800\n",
            "Processing route: 727/800\n",
            "Processing route: 728/800\n",
            "Processing route: 729/800\n",
            "Processing route: 730/800\n",
            "Processing route: 731/800\n",
            "Processing route: 732/800\n",
            "Processing route: 733/800\n",
            "Processing route: 734/800\n",
            "Processing route: 735/800\n",
            "Processing route: 736/800\n",
            "Processing route: 737/800\n",
            "Processing route: 738/800\n",
            "Processing route: 739/800\n",
            "Processing route: 740/800\n",
            "Processing route: 741/800\n",
            "Processing route: 742/800\n",
            "Processing route: 743/800\n",
            "Processing route: 744/800\n",
            "Processing route: 745/800\n",
            "Processing route: 746/800\n",
            "Processing route: 747/800\n",
            "Processing route: 748/800\n",
            "Processing route: 749/800\n",
            "Processing route: 750/800\n",
            "Processing route: 751/800\n",
            "Processing route: 752/800\n",
            "Processing route: 753/800\n",
            "Processing route: 754/800\n",
            "Processing route: 755/800\n",
            "Processing route: 756/800\n",
            "Processing route: 757/800\n",
            "Processing route: 758/800\n",
            "Processing route: 759/800\n",
            "Processing route: 760/800\n",
            "Processing route: 761/800\n",
            "Processing route: 762/800\n",
            "Processing route: 763/800\n",
            "Processing route: 764/800\n",
            "Processing route: 765/800\n",
            "Processing route: 766/800\n",
            "Processing route: 767/800\n",
            "Processing route: 768/800\n",
            "Processing route: 769/800\n",
            "Processing route: 770/800\n",
            "Processing route: 771/800\n",
            "Processing route: 772/800\n",
            "Processing route: 773/800\n",
            "Processing route: 774/800\n",
            "Processing route: 775/800\n",
            "Processing route: 776/800\n",
            "Processing route: 777/800\n",
            "Processing route: 778/800\n",
            "Processing route: 779/800\n",
            "Processing route: 780/800\n",
            "Processing route: 781/800\n",
            "Processing route: 782/800\n",
            "Processing route: 783/800\n",
            "Processing route: 784/800\n",
            "Processing route: 785/800\n",
            "Processing route: 786/800\n",
            "Processing route: 787/800\n",
            "Processing route: 788/800\n",
            "Processing route: 789/800\n",
            "Processing route: 790/800\n",
            "Processing route: 791/800\n",
            "Processing route: 792/800\n",
            "Processing route: 793/800\n",
            "Processing route: 794/800\n",
            "Processing route: 795/800\n",
            "Processing route: 796/800\n",
            "Processing route: 797/800\n",
            "Processing route: 798/800\n",
            "Processing route: 799/800\n",
            "Processing route: 800/800\n",
            "Mean calculated and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# تعريف عدد الخانات لكل من المتغيرات\n",
        "LAT_BINS = 200\n",
        "LON_BINS = 300\n",
        "SOG_BINS = 30\n",
        "COG_BINS = 72\n",
        "\n",
        "# دالة لتحويل البيانات إلى مصفوفة كثيفة\n",
        "def sparse_AIS_to_dense(msgs_, num_timesteps, mmsis):\n",
        "    def create_dense_vect(msg, lat_bins=LAT_BINS, lon_bins=LON_BINS, sog_bins=SOG_BINS, cog_bins=COG_BINS):\n",
        "        lat, lon, sog, cog = msg[0], msg[1], msg[2], msg[3]\n",
        "        data_dim = lat_bins + lon_bins + sog_bins + cog_bins\n",
        "        dense_vect = np.zeros(data_dim)\n",
        "        dense_vect[int(lat * lat_bins)] = 1.0\n",
        "        dense_vect[int(lon * lon_bins) + lat_bins] = 1.0\n",
        "        dense_vect[int(sog * sog_bins) + lat_bins + lon_bins] = 1.0\n",
        "        dense_vect[int(cog * cog_bins) + lat_bins + lon_bins + sog_bins] = 1.0\n",
        "        return dense_vect\n",
        "\n",
        "    dense_msgs = []\n",
        "    for msg in msgs_:\n",
        "        dense_msgs.append(create_dense_vect(msg))\n",
        "    dense_msgs = np.array(dense_msgs)\n",
        "    return dense_msgs, num_timesteps, mmsis\n",
        "\n",
        "# دالة لحساب المتوسط من قائمة المسارات\n",
        "def calculate_mean_from_routes(routes):\n",
        "    data_dim = LAT_BINS + LON_BINS + SOG_BINS + COG_BINS\n",
        "\n",
        "    # تهيئة المتغيرات لحساب المتوسط\n",
        "    sum_all = np.zeros((data_dim,))\n",
        "    total_ais_msg = 0\n",
        "\n",
        "    # حساب المتوسط\n",
        "    for count, route in enumerate(routes):\n",
        "        print(f\"Processing route: {count + 1}/{len(routes)}\")\n",
        "\n",
        "        # معالجة البيانات\n",
        "        current_sparse_matrix, _, _ = sparse_AIS_to_dense(route, 0, 0)\n",
        "\n",
        "        # جمع البيانات\n",
        "        sum_all += np.sum(current_sparse_matrix, axis=0)\n",
        "        total_ais_msg += len(current_sparse_matrix)\n",
        "\n",
        "    # حساب المتوسط النهائي\n",
        "    mean = sum_all / total_ais_msg\n",
        "    return mean\n",
        "\n",
        "# مثال على كيفية استخدام الدالة مع قائمة من المسارات\n",
        "if __name__ == \"__main__\":\n",
        "    # استبدل هذا بقائمة من المسارات الخاصة بك\n",
        "    routes = data\n",
        "\n",
        "    mean = calculate_mean_from_routes(routes)\n",
        "\n",
        "    # حفظ المتوسط في ملف\n",
        "\n",
        "    print(\"Mean calculated and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33wnTqlEQ2Bg",
        "outputId": "d5485561-8194-4ad9-dc9d-b7f728a3a1f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finsh get_four_hot_all_data 800\n",
            "finsh processdata\n",
            "Size of dataset: 800\n",
            "Total bins: 602\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import concurrent.futures\n",
        "\n",
        "LAT, LON, SOG, COG, HEADING, TIMESTAMP = list(range(6))\n",
        "\n",
        "class AISDataset(Dataset):\n",
        "    def __init__(self, tracks, lat_bins, lon_bins, sog_bins, cog_bins,mean,divm=360):\n",
        "        self.tracks = tracks  # قائمة من المسارات\n",
        "        self.lat_bins = lat_bins\n",
        "        self.lon_bins = lon_bins\n",
        "        self.sog_bins = sog_bins\n",
        "        self.cog_bins = cog_bins\n",
        "        self.total_bins = lat_bins + lon_bins + sog_bins + cog_bins  # إجمالي عدد الخانات\n",
        "\n",
        "        # Normalize the mean values for inputs.\n",
        "        self.mean = mean#self.calculate_mean()\n",
        "        self.dataFourHot=[]\n",
        "        self.dataTrackdFourHot=[]\n",
        "        self.processdata(divm)\n",
        "\n",
        "\n",
        "    def processdata(self,divm=360):\n",
        "       outdata=self.get_four_hot_all_data(divm=divm)\n",
        "       self.dataTrackdFourHot=outdata\n",
        "       print('finsh get_four_hot_all_data',len(outdata))\n",
        "       self.dataFourHot=[]\n",
        "       for  i in range(len(outdata)):\n",
        "         for j in range(len(outdata[i])):\n",
        "           self.dataFourHot.append(outdata[i][j,:])\n",
        "\n",
        "       print('finsh processdata')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_four_hot_all_data(self, divm=360):\n",
        "      results = []\n",
        "      for i in range(len(self.tracks)):\n",
        "              results.append(self.sparse_AIS_to_dense(self.tracks[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      return results\n",
        "\n",
        "    def sparse_AIS_to_dense_all(self,msgs_,divm=360):\n",
        "        msgs_=msgs_/divm\n",
        "        def create_dense_vect(msg, lat_bins=LAT_BINS, lon_bins=LON_BINS, sog_bins=SOG_BINS, cog_bins=COG_BINS):\n",
        "            lat, lon, sog, cog = msg[0], msg[1], msg[2], msg[3]\n",
        "            data_dim = lat_bins + lon_bins + sog_bins + cog_bins\n",
        "            dense_vect = np.zeros(data_dim)\n",
        "            dense_vect[int(lat * lat_bins)] = 1.0\n",
        "            dense_vect[int(lon * lon_bins) + lat_bins] = 1.0\n",
        "            dense_vect[int(sog * sog_bins) + lat_bins + lon_bins] = 1.0\n",
        "            dense_vect[int(cog * cog_bins) + lat_bins + lon_bins + sog_bins] = 1.0\n",
        "            return dense_vect\n",
        "\n",
        "        # التعامل مع القيم 1 في الرسالة\n",
        "        msgs_[msgs_ == 1] = 0.99999\n",
        "\n",
        "        dense_msgs = []\n",
        "        for msg in msgs_:\n",
        "            dense_msgs.append(create_dense_vect(msg))\n",
        "\n",
        "        dense_msgs = np.array(dense_msgs)\n",
        "        return dense_msgs\n",
        "\n",
        "    def sparse_AIS_to_dense_ones(self, msg):\n",
        "        dense_vect = np.zeros(self.total_bins)\n",
        "        dense_vect[int(msg[LAT] * self.lat_bins)] = 1.0\n",
        "        dense_vect[int(msg[LON] * self.lon_bins) + self.lat_bins] = 1.0\n",
        "        dense_vect[int(msg[SOG] * self.sog_bins) + self.lat_bins + self.lon_bins] = 1.0\n",
        "        dense_vect[int(msg[COG] * self.cog_bins) + self.lat_bins + self.lon_bins + self.sog_bins] = 1.0\n",
        "        return dense_vect\n",
        "    def sparse_AIS_to_dense(self, msg):\n",
        "        dense_vect = np.zeros((len(msg),self.total_bins))\n",
        "        dense_vect[:,np.int16(msg[:,LAT] * self.lat_bins)] = 1.0\n",
        "        dense_vect[:,np.int16(msg[:,LON] * self.lon_bins) + self.lat_bins] = 1.0\n",
        "        dense_vect[:,np.int16(msg[:,SOG] * self.sog_bins) + self.lat_bins + self.lon_bins] = 1.0\n",
        "        dense_vect[:,np.int16(msg[:,COG] * self.cog_bins) + self.lat_bins + self.lon_bins + self.sog_bins] = 1.0\n",
        "        return dense_vect\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataTrackdFourHot)  # عدد الرسائل في جميع المسارات\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "          idx=-1\n",
        "\n",
        "\n",
        "        inputs =self.dataTrackdFourHot[idx]-self.mean\n",
        "\n",
        "\n",
        "        # Normalize inputs by subtracting mean\n",
        "\n",
        "\n",
        "        return torch.FloatTensor(inputs)\n",
        "def create_dataloader(tracks, batch_size, lat_bins, lon_bins, sog_bins, cog_bins, shuffle=True):\n",
        "    dataset = AISDataset(tracks, lat_bins, lon_bins, sog_bins, cog_bins)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return dataloader\n",
        "\n",
        "# مثال على كيفية استخدام الدالة لإنشاء DataLoader\n",
        "# استبدل هذا بمساراتك الفعلية\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# تعريف عدد الخانات لكل من المتغيرات\n",
        "LAT_BINS = 200\n",
        "LON_BINS = 300\n",
        "SOG_BINS = 30\n",
        "COG_BINS = 72\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "lat_bins = 200\n",
        "lon_bins = 300\n",
        "sog_bins = 30\n",
        "cog_bins = 72\n",
        "# data=[  [\n",
        "#         [0.1, 0.2, 0.3, 0.4],\n",
        "#         [0.15, 0.25, 0.35, 0.45],\n",
        "#         # أضف المزيد من الرسائل حسب الحاجة\n",
        "#     ]*10]*100\n",
        "\n",
        "# تحديد قيم size, mean, و total_bins\n",
        "dataset = AISDataset(data, lat_bins, lon_bins, sog_bins, cog_bins,mean)\n",
        "\n",
        "print(f\"Size of dataset: {len(dataset)}\")\n",
        "\n",
        "print(f\"Total bins: {dataset.total_bins}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJa6Kod0alaN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.functional import softplus\n",
        "from torch.distributions import Distribution\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "\n",
        "class VRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape, latent_shape, mean_logits, mean_, splits, len_data, gamma, bn_switch):\n",
        "        super(VRNN, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.latent_shape = latent_shape\n",
        "        self.mean_logits = mean_logits\n",
        "        self.mean_ = mean_\n",
        "        self.splits = splits\n",
        "        self.len_data = len_data\n",
        "        self.gamma = gamma\n",
        "        self.bn_switch = bn_switch\n",
        "\n",
        "        self.phi_x = nn.Sequential(nn.Linear(self.input_shape, self.latent_shape),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "        self.phi_z = nn.Sequential(nn.Linear(self.latent_shape, self.latent_shape),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "        self.prior = nn.Sequential(nn.Linear(self.latent_shape, 2*self.latent_shape),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "        self.encoder = nn.Sequential(nn.Linear(self.latent_shape + self.latent_shape, 2*self.latent_shape),\n",
        "                                     nn.ReLU())\n",
        "\n",
        "        self.decoder = nn.Sequential(nn.Linear(self.latent_shape + self.latent_shape, 2*self.input_shape),\n",
        "                                     nn.ReLU())\n",
        "\n",
        "        self.rnn = nn.LSTM(self.latent_shape + self.latent_shape, self.latent_shape, batch_first=True)\n",
        "\n",
        "        self.register_buffer('out', torch.zeros(1, self.latent_shape))\n",
        "        self.register_buffer('h', torch.zeros(1, self.latent_shape))\n",
        "        self.register_buffer('c', torch.zeros(1, 1, self.latent_shape))\n",
        "\n",
        "        if self.bn_switch:\n",
        "            self.bn = nn.BatchNorm1d(self.latent_shape)\n",
        "            self.bn.weight.requires_grad = False\n",
        "            self.bn.weight.fill_(self.gamma)\n",
        "\n",
        "    def _prior(self, h):\n",
        "        hidden = self.prior(h)\n",
        "        mu, log_sigma = hidden.chunk(2, dim=-1)\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
        "\n",
        "    def posterior(self, hidden, x, prior_mu):\n",
        "        encoder_input = torch.cat([hidden, x], dim=1)\n",
        "        hidden = self.encoder(encoder_input)\n",
        "        mu, log_sigma = hidden.chunk(2, dim=-1)\n",
        "        mu = mu + prior_mu\n",
        "        if self.bn_switch:\n",
        "            mu = self.bn(mu)\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
        "\n",
        "    def generative(self, z_enc, h):\n",
        "        decoder_output = self.decoder(torch.cat([z_enc, h], dim=1))\n",
        "        mu, log_sigma = decoder_output.chunk(2, dim=-1)\n",
        "        sigma = softplus(log_sigma)  # Ensure sigma is positive\n",
        "        return Normal(mu, sigma)\n",
        "\n",
        "    def calculate_anomaly_score(self, mu, sigma, x):\n",
        "        # Calculate the z-score\n",
        "        z_scores = (x - mu) / sigma\n",
        "        # Anomaly score is defined as the absolute z-score\n",
        "        return torch.abs(z_scores)\n",
        "\n",
        "    def calc_mi(self, inputs):\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "\n",
        "        out = self.out.expand(batch_size, *self.out.shape[1:]).contiguous()\n",
        "        h = self.h.expand(1, batch_size, self.c.shape[-1]).contiguous()\n",
        "        c = self.c.expand(1, batch_size, self.c.shape[-1]).contiguous()\n",
        "\n",
        "        neg_entropy = 0\n",
        "        log_qz = 0\n",
        "\n",
        "        for t in range(inputs.size(1)):\n",
        "            x = inputs[:, t, :]\n",
        "            x_hat = self.phi_x(x)\n",
        "\n",
        "            # Create prior distribution\n",
        "            pz = self._prior(out)\n",
        "\n",
        "            # Create approximate posterior\n",
        "            qz = self.posterior(out, x_hat, prior_mu=pz.mu)\n",
        "\n",
        "            mu = qz.mu\n",
        "            logsigma = torch.log(qz.sigma)\n",
        "\n",
        "            z = qz.rsample()\n",
        "            z_hat = self.phi_z(z)\n",
        "\n",
        "            rnn_input = torch.cat([x_hat, z_hat], dim=1)\n",
        "            rnn_input = rnn_input.unsqueeze(1)\n",
        "            out, (h, c) = self.rnn(rnn_input, (h, c))\n",
        "            out = out.squeeze()\n",
        "\n",
        "            neg_entropy += (-0.5 * self.latent_shape * math.log(2 * math.pi) - 0.5 * (1 + 2 * logsigma).sum(-1)).mean()\n",
        "\n",
        "            var = logsigma.exp()**2\n",
        "\n",
        "            z = z.unsqueeze(1)\n",
        "            mu = mu.unsqueeze(0)\n",
        "            logsigma = logsigma.unsqueeze(0)\n",
        "\n",
        "            dev = z - mu\n",
        "\n",
        "            log_density = -0.5 * ((dev ** 2 ) / var).sum(dim=-1) - 0.5 * (self.latent_shape * math.log(2 * math.pi) + (2*logsigma).sum(dim=-1))\n",
        "\n",
        "            log_qz1 = torch.logsumexp(log_density, dim=1) - math.log(batch_size)\n",
        "            log_qz += log_qz1.mean(-1)\n",
        "\n",
        "        mi = (neg_entropy / inputs.size(1)) - (log_qz / inputs.size(1))\n",
        "\n",
        "        return mi\n",
        "\n",
        "    def forward(self, inputs, beta):\n",
        "        batch_size = inputs.size(0)\n",
        "\n",
        "        out = self.out.expand(batch_size, *self.out.shape[1:]).contiguous()\n",
        "        h = self.h.expand(1, batch_size, self.c.shape[-1]).contiguous()\n",
        "        c = self.c.expand(1, batch_size, self.c.shape[-1]).contiguous()\n",
        "\n",
        "        acc_loss = 0\n",
        "        loss_list = []\n",
        "        kl_list = []\n",
        "        log_px_list = []\n",
        "        mu_prior = []\n",
        "        mu_post = []\n",
        "        noise_levels = []\n",
        "        accuracy_list = []\n",
        "\n",
        "        for t in range(inputs.size(1)):\n",
        "            x = inputs[:, t, :]\n",
        "\n",
        "\n",
        "            # Embed input\n",
        "            x_hat = self.phi_x(x)\n",
        "\n",
        "            # Create prior distribution\n",
        "            pz = self._prior(out)\n",
        "\n",
        "            # Create approximate posterior\n",
        "            qz = self.posterior(out, x_hat, prior_mu=pz.mu)\n",
        "\n",
        "            # Sample and embed z from posterior\n",
        "            z = qz.rsample()\n",
        "            z_hat = self.phi_z(z)\n",
        "\n",
        "            # Decode z_hat\n",
        "            px = self.generative(z_hat, out)\n",
        "\n",
        "            # Update h from LSTM\n",
        "            rnn_input = torch.cat([x_hat, z_hat], dim=1)\n",
        "            rnn_input = rnn_input.unsqueeze(1)\n",
        "            out, (h, c) = self.rnn(rnn_input, (h, c))\n",
        "            out = out.squeeze()\n",
        "\n",
        "            # Calculate loss\n",
        "            log_px = px.log_prob(x).sum(dim=1)\n",
        "            log_pz = pz.log_prob(z).sum(dim=1)\n",
        "            log_qz = qz.log_prob(z).sum(dim=1)\n",
        "\n",
        "            kl = log_qz - log_pz\n",
        "            elbo_beta = log_px - beta * kl\n",
        "\n",
        "            acc_loss += elbo_beta\n",
        "            loss_list.append(-elbo_beta)\n",
        "            kl_list.append(kl)\n",
        "            mu_prior.append(pz.mu.sum(dim=1))\n",
        "            mu_post.append(qz.mu.sum(dim=1))\n",
        "\n",
        "            # Calculate noise level as the standard deviation of the predicted output\n",
        "            noise_level = px.stddev.sum(dim=1)\n",
        "            noise_levels.append(noise_level)\n",
        "\n",
        "            # Calculate anomaly score\n",
        "            anomaly_score = self.calculate_anomaly_score(px.mean, px.stddev, x)\n",
        "            accuracy_list.append(anomaly_score)\n",
        "\n",
        "        loss = torch.mean(acc_loss / inputs.size(1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            diagnostics = {\n",
        "                'loss_list': torch.stack(loss_list).cpu().numpy(),\n",
        "                # 'log_px': torch.stack(log_px_list).cpu().numpy(),\n",
        "                'kl': torch.stack(kl_list).cpu().numpy(),\n",
        "                \"mu_prior\": torch.stack(mu_prior).cpu().numpy(),\n",
        "                \"mu_post\": torch.stack(mu_post).cpu().numpy(),\n",
        "                \"noise_levels\": torch.stack(noise_levels).cpu().numpy(),\n",
        "                \"accuracy\": torch.stack(accuracy_list).cpu().numpy(),\n",
        "            }\n",
        "\n",
        "        return -loss, diagnostics\n",
        "\n",
        "class ReparameterizedDiagonalGaussian(Distribution):\n",
        "    def __init__(self, mu: Tensor, log_sigma: Tensor):\n",
        "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
        "        self.mu = mu\n",
        "        self.sigma = log_sigma.exp()\n",
        "\n",
        "    def sample_epsilon(self) -> Tensor:\n",
        "        return torch.empty_like(self.mu).normal_()\n",
        "\n",
        "    def sample(self) -> Tensor:\n",
        "        with torch.no_grad():\n",
        "            return self.rsample()\n",
        "\n",
        "    def rsample(self) -> Tensor:\n",
        "        return self.mu + self.sigma * self.sample_epsilon()\n",
        "\n",
        "    def log_prob(self, z: Tensor) -> Tensor:\n",
        "        dist = Normal(self.mu, self.sigma)\n",
        "        return dist.log_prob(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okvKcLVvpise"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIlDbB87b1CR",
        "outputId": "1b3e944f-b7ff-4e7d-b740-9e6f420d9486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwtgIrZTb5ok",
        "outputId": "faa9162a-e860-4b8c-f559-9c057ae031d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhmamyr743\u001b[0m (\u001b[33mmhmamyr743-wew\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"dd5a000648e992f5fd8fb3efad3966fe8b8b671f\")  # Replace with your API key\n",
        "# Initialize WandB\n",
        "#dd5a000648e992f5fd8fb3efad3966fe8b8b671f"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jAV_acYnz6XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e13f0ed83a0d4fc3a158892f11ae028d",
            "991e47a0e1d04da9967e6aa230a275a9",
            "375717516f60497f91186d4f4d37d2c7",
            "157478c8abf1431997b12934d7a38f91",
            "2733f42ed517410a84db8e1aa35c6769",
            "9f4c82a3f9b44fe9b40af157eed13068",
            "560d9a8614d14314b1726dc49bfc3c4a",
            "0279620742cd406a86f716f6b1560e71"
          ]
        },
        "id": "59nNCSP9lOUi",
        "outputId": "831c3dee-f494-44e5-da37-2962b57e0596"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113613633333823, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e13f0ed83a0d4fc3a158892f11ae028d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241226_202016-cszjw0uo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mhmamyr743-wew/myproject/runs/cszjw0uo' target=\"_blank\">daily-fire-53</a></strong> to <a href='https://wandb.ai/mhmamyr743-wew/myproject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mhmamyr743-wew/myproject' target=\"_blank\">https://wandb.ai/mhmamyr743-wew/myproject</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mhmamyr743-wew/myproject/runs/cszjw0uo' target=\"_blank\">https://wandb.ai/mhmamyr743-wew/myproject/runs/cszjw0uo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Train Loss: 378.3473, kl :0.6641979217529297, Validation Loss: 371.8550, Test Loss: 370.8964\n",
            "Epoch 2/200, Train Loss: 362.2416, kl :0.6346280574798584, Validation Loss: 358.0604, Test Loss: 357.1312\n",
            "Epoch 3/200, Train Loss: 351.4659, kl :0.4980340301990509, Validation Loss: 349.0939, Test Loss: 348.2196\n",
            "Epoch 4/200, Train Loss: 345.2932, kl :0.3337443470954895, Validation Loss: 344.7869, Test Loss: 343.7960\n",
            "Epoch 5/200, Train Loss: 342.0004, kl :0.2716023325920105, Validation Loss: 342.5914, Test Loss: 341.4882\n",
            "Epoch 6/200, Train Loss: 340.2277, kl :0.24269886314868927, Validation Loss: 341.7125, Test Loss: 340.5168\n",
            "Epoch 7/200, Train Loss: 339.8735, kl :0.25246214866638184, Validation Loss: 379.4231, Test Loss: 379.0664\n",
            "Epoch 8/200, Train Loss: 339.1463, kl :0.2320970743894577, Validation Loss: 341.4349, Test Loss: 340.2191\n",
            "Epoch 9/200, Train Loss: 339.6302, kl :0.20704437792301178, Validation Loss: 341.2012, Test Loss: 339.9948\n",
            "Epoch 10/200, Train Loss: 339.3866, kl :0.20604144036769867, Validation Loss: 396.2113, Test Loss: 395.1874\n",
            "Epoch 11/200, Train Loss: 339.3728, kl :0.19295479357242584, Validation Loss: 340.9234, Test Loss: 339.7200\n",
            "Epoch 12/200, Train Loss: 339.1115, kl :0.19138512015342712, Validation Loss: 371.5306, Test Loss: 372.0855\n",
            "Epoch 13/200, Train Loss: 338.8803, kl :0.19099484384059906, Validation Loss: 372.5443, Test Loss: 372.3844\n",
            "Epoch 14/200, Train Loss: 338.9058, kl :0.18291844427585602, Validation Loss: 369.9188, Test Loss: 370.1208\n",
            "Epoch 15/200, Train Loss: 339.2373, kl :0.17654958367347717, Validation Loss: 370.5784, Test Loss: 370.8898\n",
            "Epoch 16/200, Train Loss: 338.9963, kl :0.16471430659294128, Validation Loss: 367.8831, Test Loss: 367.2457\n",
            "Epoch 17/200, Train Loss: 339.1917, kl :0.16195596754550934, Validation Loss: 376.3045, Test Loss: 375.7337\n",
            "Epoch 18/200, Train Loss: 339.0487, kl :0.15225018560886383, Validation Loss: 360.2127, Test Loss: 358.1036\n",
            "Epoch 19/200, Train Loss: 338.5979, kl :0.1575743556022644, Validation Loss: 340.5141, Test Loss: 339.2914\n",
            "Epoch 20/200, Train Loss: 338.7642, kl :0.15712589025497437, Validation Loss: 374.2277, Test Loss: 374.3552\n",
            "Epoch 21/200, Train Loss: 338.6354, kl :0.13132339715957642, Validation Loss: 381.1418, Test Loss: 379.1153\n",
            "Epoch 22/200, Train Loss: 338.6775, kl :0.14547988772392273, Validation Loss: 375.1056, Test Loss: 373.1142\n",
            "Epoch 23/200, Train Loss: 338.4229, kl :0.125895157456398, Validation Loss: 340.1133, Test Loss: 338.9601\n",
            "Epoch 24/200, Train Loss: 338.5776, kl :0.12042955309152603, Validation Loss: 340.0672, Test Loss: 338.9341\n",
            "Epoch 25/200, Train Loss: 338.3047, kl :0.13156954944133759, Validation Loss: 379.0285, Test Loss: 376.5955\n",
            "Epoch 26/200, Train Loss: 338.5007, kl :0.11938070505857468, Validation Loss: 339.9159, Test Loss: 338.7939\n",
            "Epoch 27/200, Train Loss: 338.3857, kl :0.11986629664897919, Validation Loss: 372.0390, Test Loss: 369.3163\n",
            "Epoch 28/200, Train Loss: 338.2909, kl :0.12766209244728088, Validation Loss: 374.2353, Test Loss: 372.9499\n",
            "Epoch 29/200, Train Loss: 338.0110, kl :0.10508892685174942, Validation Loss: 340.0415, Test Loss: 338.9596\n",
            "Epoch 30/200, Train Loss: 338.1227, kl :0.10831521451473236, Validation Loss: 339.6892, Test Loss: 338.6195\n",
            "Epoch 31/200, Train Loss: 337.8312, kl :0.12320667505264282, Validation Loss: 392.9884, Test Loss: 384.2639\n",
            "Epoch 32/200, Train Loss: 337.6419, kl :0.1056424006819725, Validation Loss: 339.5202, Test Loss: 338.4487\n",
            "Epoch 33/200, Train Loss: 338.0249, kl :0.10672733187675476, Validation Loss: 339.4695, Test Loss: 338.4117\n",
            "Epoch 34/200, Train Loss: 337.9854, kl :0.10088501125574112, Validation Loss: 339.3201, Test Loss: 338.2827\n",
            "Epoch 35/200, Train Loss: 337.5609, kl :0.1129278689622879, Validation Loss: 385.3582, Test Loss: 379.6267\n",
            "Epoch 36/200, Train Loss: 337.5388, kl :0.09698416292667389, Validation Loss: 339.3185, Test Loss: 338.2945\n",
            "Epoch 37/200, Train Loss: 337.3809, kl :0.10225461423397064, Validation Loss: 339.1359, Test Loss: 338.1519\n",
            "Epoch 38/200, Train Loss: 337.6280, kl :0.10160860419273376, Validation Loss: 339.0634, Test Loss: 338.0868\n",
            "Epoch 39/200, Train Loss: 337.8193, kl :0.09916864335536957, Validation Loss: 339.0593, Test Loss: 338.0811\n",
            "Epoch 40/200, Train Loss: 337.5619, kl :0.08495193719863892, Validation Loss: 339.0838, Test Loss: 338.0720\n",
            "Epoch 41/200, Train Loss: 337.5545, kl :0.10279029607772827, Validation Loss: 390.9409, Test Loss: 383.5652\n",
            "Epoch 42/200, Train Loss: 337.8538, kl :0.09684797376394272, Validation Loss: 389.5898, Test Loss: 383.8090\n",
            "Epoch 43/200, Train Loss: 337.1823, kl :0.0911521166563034, Validation Loss: 339.4764, Test Loss: 338.3763\n",
            "Epoch 44/200, Train Loss: 337.4206, kl :0.1033523827791214, Validation Loss: 405.2747, Test Loss: 399.0911\n",
            "Epoch 45/200, Train Loss: 337.5962, kl :0.0925883874297142, Validation Loss: 403.1619, Test Loss: 397.7748\n",
            "Epoch 46/200, Train Loss: 337.5143, kl :0.07351207733154297, Validation Loss: 338.7873, Test Loss: 337.8316\n",
            "Epoch 47/200, Train Loss: 337.1915, kl :0.08842119574546814, Validation Loss: 381.2591, Test Loss: 380.7621\n",
            "Epoch 48/200, Train Loss: 337.1592, kl :0.09051258116960526, Validation Loss: 366.4074, Test Loss: 362.1231\n",
            "Epoch 49/200, Train Loss: 336.8803, kl :0.08092240989208221, Validation Loss: 396.7387, Test Loss: 390.7181\n",
            "Epoch 50/200, Train Loss: 337.0499, kl :0.07631760090589523, Validation Loss: 365.4961, Test Loss: 361.6114\n",
            "Epoch 51/200, Train Loss: 337.3027, kl :0.07269543409347534, Validation Loss: 390.2791, Test Loss: 386.5522\n",
            "Epoch 52/200, Train Loss: 337.1370, kl :0.0701131820678711, Validation Loss: 338.2889, Test Loss: 337.4054\n",
            "Epoch 53/200, Train Loss: 337.0633, kl :0.0735580325126648, Validation Loss: 366.7618, Test Loss: 367.8966\n",
            "Epoch 54/200, Train Loss: 337.1150, kl :0.06974651664495468, Validation Loss: 339.6060, Test Loss: 338.6694\n",
            "Epoch 55/200, Train Loss: 337.0243, kl :0.05696573480963707, Validation Loss: 338.9109, Test Loss: 338.0316\n",
            "Epoch 56/200, Train Loss: 336.8220, kl :0.06113418936729431, Validation Loss: 338.3794, Test Loss: 337.5381\n",
            "Epoch 57/200, Train Loss: 337.1049, kl :0.05067698657512665, Validation Loss: 338.0745, Test Loss: 337.2205\n",
            "Epoch 58/200, Train Loss: 336.9283, kl :0.05913105234503746, Validation Loss: 402.0047, Test Loss: 397.9460\n",
            "Epoch 59/200, Train Loss: 337.3294, kl :0.06766904890537262, Validation Loss: 412.0171, Test Loss: 405.6569\n",
            "Epoch 60/200, Train Loss: 336.8900, kl :0.05708223581314087, Validation Loss: 397.0638, Test Loss: 394.1939\n",
            "Epoch 61/200, Train Loss: 336.9535, kl :0.06385160982608795, Validation Loss: 420.6718, Test Loss: 409.7204\n",
            "Epoch 62/200, Train Loss: 336.8827, kl :0.06221459060907364, Validation Loss: 339.3654, Test Loss: 338.6302\n",
            "Epoch 63/200, Train Loss: 336.8940, kl :0.05117062106728554, Validation Loss: 339.0659, Test Loss: 338.4166\n",
            "Epoch 64/200, Train Loss: 336.4215, kl :0.05689936876296997, Validation Loss: 403.0519, Test Loss: 397.2142\n",
            "Epoch 65/200, Train Loss: 336.6030, kl :0.05039270222187042, Validation Loss: 338.3144, Test Loss: 337.5035\n",
            "Epoch 66/200, Train Loss: 336.5467, kl :0.04837343841791153, Validation Loss: 339.8961, Test Loss: 338.2361\n",
            "Epoch 67/200, Train Loss: 336.4717, kl :0.0626591369509697, Validation Loss: 340.9171, Test Loss: 339.6601\n",
            "Epoch 68/200, Train Loss: 336.5063, kl :0.049335990101099014, Validation Loss: 338.5917, Test Loss: 337.4898\n",
            "Epoch 69/200, Train Loss: 336.5558, kl :0.043109845370054245, Validation Loss: 339.7070, Test Loss: 339.0214\n",
            "Epoch 70/200, Train Loss: 336.5693, kl :0.05630337446928024, Validation Loss: 338.5263, Test Loss: 337.9443\n",
            "Epoch 71/200, Train Loss: 336.5146, kl :0.042365662753582, Validation Loss: 338.1601, Test Loss: 337.5125\n",
            "Epoch 72/200, Train Loss: 336.3943, kl :0.04273218289017677, Validation Loss: 410.5582, Test Loss: 405.1826\n",
            "Epoch 73/200, Train Loss: 336.1131, kl :0.04457710683345795, Validation Loss: 339.7567, Test Loss: 338.3863\n",
            "Epoch 74/200, Train Loss: 336.6114, kl :0.03591995686292648, Validation Loss: 339.9704, Test Loss: 338.2071\n",
            "Epoch 75/200, Train Loss: 336.3345, kl :0.04237945377826691, Validation Loss: 422.2037, Test Loss: 414.0535\n",
            "Epoch 76/200, Train Loss: 336.3279, kl :0.03644997626543045, Validation Loss: 339.0339, Test Loss: 338.5944\n",
            "Epoch 77/200, Train Loss: 336.2790, kl :0.044854070991277695, Validation Loss: 409.1202, Test Loss: 404.6120\n",
            "Epoch 78/200, Train Loss: 336.5221, kl :0.03832356631755829, Validation Loss: 337.2339, Test Loss: 336.4981\n",
            "Epoch 79/200, Train Loss: 336.1004, kl :0.04220074415206909, Validation Loss: 407.5149, Test Loss: 403.9118\n",
            "Epoch 80/200, Train Loss: 336.1356, kl :0.03644288703799248, Validation Loss: 337.9330, Test Loss: 337.0937\n",
            "Epoch 81/200, Train Loss: 336.3835, kl :0.04092266410589218, Validation Loss: 339.3730, Test Loss: 337.7826\n",
            "Epoch 82/200, Train Loss: 336.2707, kl :0.03822268918156624, Validation Loss: 415.6581, Test Loss: 414.8883\n",
            "Epoch 83/200, Train Loss: 336.2113, kl :0.03938158601522446, Validation Loss: 412.2359, Test Loss: 409.6042\n",
            "Epoch 84/200, Train Loss: 335.9885, kl :0.03817639499902725, Validation Loss: 339.1770, Test Loss: 337.6288\n",
            "Epoch 85/200, Train Loss: 335.9088, kl :0.041426267474889755, Validation Loss: 421.7741, Test Loss: 417.3546\n",
            "Epoch 86/200, Train Loss: 335.9479, kl :0.040198471397161484, Validation Loss: 415.8220, Test Loss: 412.5543\n",
            "Epoch 87/200, Train Loss: 335.8508, kl :0.037422288209199905, Validation Loss: 412.2831, Test Loss: 407.6822\n",
            "Epoch 88/200, Train Loss: 336.1764, kl :0.0469188429415226, Validation Loss: 339.9845, Test Loss: 339.1576\n",
            "Epoch 89/200, Train Loss: 335.7702, kl :0.03821830824017525, Validation Loss: 404.7324, Test Loss: 403.7159\n",
            "Epoch 90/200, Train Loss: 336.0787, kl :0.04637846350669861, Validation Loss: 339.1132, Test Loss: 338.6662\n",
            "Epoch 91/200, Train Loss: 336.1451, kl :0.036417461931705475, Validation Loss: 339.0925, Test Loss: 338.5525\n",
            "Epoch 92/200, Train Loss: 335.8169, kl :0.04357486590743065, Validation Loss: 337.6749, Test Loss: 336.6709\n",
            "Epoch 93/200, Train Loss: 335.8941, kl :0.03545074909925461, Validation Loss: 340.8406, Test Loss: 339.5471\n",
            "Epoch 94/200, Train Loss: 335.7629, kl :0.03713540732860565, Validation Loss: 336.9176, Test Loss: 336.2412\n",
            "Epoch 95/200, Train Loss: 335.6878, kl :0.035319581627845764, Validation Loss: 340.7384, Test Loss: 339.3083\n",
            "Epoch 96/200, Train Loss: 335.7445, kl :0.03440552577376366, Validation Loss: 339.2294, Test Loss: 337.6013\n",
            "Epoch 97/200, Train Loss: 335.8115, kl :0.042574599385261536, Validation Loss: 408.0361, Test Loss: 403.8556\n",
            "Epoch 98/200, Train Loss: 335.7629, kl :0.02840864472091198, Validation Loss: 336.5785, Test Loss: 335.9680\n",
            "Epoch 99/200, Train Loss: 335.6894, kl :0.03406005725264549, Validation Loss: 339.7234, Test Loss: 338.6479\n",
            "Epoch 100/200, Train Loss: 335.6495, kl :0.04070073738694191, Validation Loss: 338.3982, Test Loss: 337.3138\n",
            "Epoch 101/200, Train Loss: 335.5865, kl :0.04445477947592735, Validation Loss: 391.7514, Test Loss: 392.1613\n",
            "Epoch 102/200, Train Loss: 335.6528, kl :0.03678569570183754, Validation Loss: 339.6957, Test Loss: 338.1406\n",
            "Epoch 103/200, Train Loss: 335.7300, kl :0.03375957906246185, Validation Loss: 336.4085, Test Loss: 335.8130\n",
            "Epoch 104/200, Train Loss: 335.4858, kl :0.03821719065308571, Validation Loss: 337.4495, Test Loss: 336.5874\n",
            "Epoch 105/200, Train Loss: 335.5070, kl :0.03959193453192711, Validation Loss: 433.3871, Test Loss: 428.4039\n",
            "Epoch 106/200, Train Loss: 335.4907, kl :0.036428969353437424, Validation Loss: 343.1150, Test Loss: 341.0091\n",
            "Epoch 107/200, Train Loss: 335.5277, kl :0.03774693235754967, Validation Loss: 336.5454, Test Loss: 335.9214\n",
            "Epoch 108/200, Train Loss: 335.3002, kl :0.03350808843970299, Validation Loss: 337.5806, Test Loss: 336.8115\n",
            "Epoch 109/200, Train Loss: 335.5764, kl :0.0370495431125164, Validation Loss: 337.5813, Test Loss: 336.5104\n",
            "Epoch 110/200, Train Loss: 335.4872, kl :0.035474229604005814, Validation Loss: 431.3596, Test Loss: 426.7900\n",
            "Epoch 111/200, Train Loss: 335.5354, kl :0.04415673762559891, Validation Loss: 339.5785, Test Loss: 338.4517\n",
            "Epoch 112/200, Train Loss: 335.4602, kl :0.04084019362926483, Validation Loss: 338.5781, Test Loss: 337.6159\n",
            "Epoch 113/200, Train Loss: 335.5310, kl :0.03243301063776016, Validation Loss: 410.7713, Test Loss: 406.1309\n",
            "Epoch 114/200, Train Loss: 335.4242, kl :0.03640669584274292, Validation Loss: 340.1725, Test Loss: 338.3750\n",
            "Epoch 115/200, Train Loss: 335.4497, kl :0.03748128563165665, Validation Loss: 409.7282, Test Loss: 404.9142\n",
            "Epoch 116/200, Train Loss: 335.4369, kl :0.032511111348867416, Validation Loss: 339.7351, Test Loss: 338.0703\n",
            "Epoch 117/200, Train Loss: 335.4186, kl :0.03318674862384796, Validation Loss: 421.1134, Test Loss: 415.9292\n",
            "Epoch 118/200, Train Loss: 335.3340, kl :0.024759571999311447, Validation Loss: 412.4714, Test Loss: 409.0212\n",
            "Epoch 119/200, Train Loss: 335.3016, kl :0.040351878851652145, Validation Loss: 338.6837, Test Loss: 337.4623\n",
            "Epoch 120/200, Train Loss: 335.2307, kl :0.030506597831845284, Validation Loss: 422.4164, Test Loss: 422.3287\n",
            "Epoch 121/200, Train Loss: 335.3477, kl :0.02461521327495575, Validation Loss: 426.2724, Test Loss: 427.9815\n",
            "Epoch 122/200, Train Loss: 335.2570, kl :0.0427936390042305, Validation Loss: 336.4095, Test Loss: 335.7895\n",
            "Epoch 123/200, Train Loss: 335.3992, kl :0.03731676563620567, Validation Loss: 339.5573, Test Loss: 337.7831\n",
            "Epoch 124/200, Train Loss: 335.1879, kl :0.034480366855859756, Validation Loss: 336.5734, Test Loss: 335.9056\n",
            "Epoch 125/200, Train Loss: 335.2827, kl :0.050552867352962494, Validation Loss: 369.8252, Test Loss: 369.5294\n",
            "Epoch 126/200, Train Loss: 335.2266, kl :0.03863547742366791, Validation Loss: 338.8821, Test Loss: 337.3358\n",
            "Epoch 127/200, Train Loss: 335.2511, kl :0.04419803246855736, Validation Loss: 336.6739, Test Loss: 336.0224\n",
            "Epoch 128/200, Train Loss: 335.2005, kl :0.03597663342952728, Validation Loss: 338.6162, Test Loss: 337.2703\n",
            "Epoch 129/200, Train Loss: 335.1043, kl :0.034470707178115845, Validation Loss: 338.0169, Test Loss: 336.9525\n",
            "Epoch 130/200, Train Loss: 335.0254, kl :0.03285489231348038, Validation Loss: 414.9384, Test Loss: 409.6450\n",
            "Epoch 131/200, Train Loss: 335.1270, kl :0.028991907835006714, Validation Loss: 427.4941, Test Loss: 422.7129\n",
            "Epoch 132/200, Train Loss: 335.1551, kl :0.03321632370352745, Validation Loss: 410.3867, Test Loss: 403.8412\n",
            "Epoch 133/200, Train Loss: 335.0071, kl :0.03733382374048233, Validation Loss: 339.6982, Test Loss: 337.7936\n",
            "Epoch 134/200, Train Loss: 335.1139, kl :0.030093591660261154, Validation Loss: 414.8007, Test Loss: 407.1401\n",
            "Epoch 135/200, Train Loss: 334.9371, kl :0.046333570033311844, Validation Loss: 340.1966, Test Loss: 338.2137\n",
            "Epoch 136/200, Train Loss: 334.8724, kl :0.03399801254272461, Validation Loss: 426.3802, Test Loss: 424.9241\n",
            "Epoch 137/200, Train Loss: 334.9166, kl :0.043447013944387436, Validation Loss: 338.0164, Test Loss: 336.9330\n",
            "Epoch 138/200, Train Loss: 334.9157, kl :0.03015553206205368, Validation Loss: 336.1580, Test Loss: 335.6641\n",
            "Epoch 139/200, Train Loss: 334.9696, kl :0.028717374429106712, Validation Loss: 425.9858, Test Loss: 420.2551\n",
            "Epoch 140/200, Train Loss: 334.9003, kl :0.028784889727830887, Validation Loss: 428.7047, Test Loss: 426.8838\n",
            "Epoch 141/200, Train Loss: 335.0934, kl :0.03515630215406418, Validation Loss: 339.0347, Test Loss: 337.4234\n",
            "Epoch 142/200, Train Loss: 334.8959, kl :0.029185738414525986, Validation Loss: 337.7075, Test Loss: 336.7445\n",
            "Epoch 143/200, Train Loss: 335.0518, kl :0.03348901495337486, Validation Loss: 339.7097, Test Loss: 337.9877\n",
            "Epoch 144/200, Train Loss: 334.9904, kl :0.03189114108681679, Validation Loss: 395.5112, Test Loss: 390.1238\n",
            "Epoch 145/200, Train Loss: 334.8628, kl :0.026667732745409012, Validation Loss: 420.2345, Test Loss: 413.7215\n",
            "Epoch 146/200, Train Loss: 334.7653, kl :0.03100046142935753, Validation Loss: 337.8373, Test Loss: 336.5797\n",
            "Epoch 147/200, Train Loss: 334.8042, kl :0.033056966960430145, Validation Loss: 425.7107, Test Loss: 424.3386\n",
            "Epoch 148/200, Train Loss: 335.0214, kl :0.04407956451177597, Validation Loss: 337.7579, Test Loss: 336.7260\n",
            "Epoch 149/200, Train Loss: 334.8215, kl :0.025707362219691277, Validation Loss: 417.9655, Test Loss: 411.3375\n",
            "Epoch 150/200, Train Loss: 334.7163, kl :0.030470309779047966, Validation Loss: 420.4572, Test Loss: 424.1682\n",
            "Epoch 151/200, Train Loss: 334.8329, kl :0.03499789908528328, Validation Loss: 383.0598, Test Loss: 378.3167\n",
            "Epoch 152/200, Train Loss: 334.8317, kl :0.028213290497660637, Validation Loss: 422.4297, Test Loss: 422.5034\n",
            "Epoch 153/200, Train Loss: 334.7411, kl :0.03652368485927582, Validation Loss: 335.7276, Test Loss: 335.2615\n",
            "Epoch 154/200, Train Loss: 334.7524, kl :0.03422988951206207, Validation Loss: 416.5608, Test Loss: 406.9887\n",
            "Epoch 155/200, Train Loss: 334.7711, kl :0.0439174585044384, Validation Loss: 335.4801, Test Loss: 335.0124\n",
            "Epoch 156/200, Train Loss: 334.8368, kl :0.0752824991941452, Validation Loss: 337.2658, Test Loss: 336.2818\n",
            "Epoch 157/200, Train Loss: 334.8804, kl :0.07874266803264618, Validation Loss: 437.8987, Test Loss: 433.2944\n",
            "Epoch 158/200, Train Loss: 334.6513, kl :0.047506604343652725, Validation Loss: 339.7607, Test Loss: 337.7695\n",
            "Epoch 159/200, Train Loss: 334.6462, kl :0.05148885026574135, Validation Loss: 336.1423, Test Loss: 335.7259\n",
            "Epoch 160/200, Train Loss: 334.6169, kl :0.053341761231422424, Validation Loss: 490.3867, Test Loss: 475.6474\n",
            "Epoch 161/200, Train Loss: 334.7139, kl :0.054343633353710175, Validation Loss: 339.9236, Test Loss: 338.6614\n",
            "Epoch 162/200, Train Loss: 334.6782, kl :0.04495345801115036, Validation Loss: 483.4317, Test Loss: 463.4272\n",
            "Epoch 163/200, Train Loss: 334.6623, kl :0.04190396890044212, Validation Loss: 478.9884, Test Loss: 464.5675\n",
            "Epoch 164/200, Train Loss: 334.6700, kl :0.043921735137701035, Validation Loss: 442.9946, Test Loss: 434.1075\n",
            "Epoch 165/200, Train Loss: 334.8716, kl :0.045094940811395645, Validation Loss: 470.5141, Test Loss: 462.2829\n",
            "Epoch 166/200, Train Loss: 334.6966, kl :0.04157088324427605, Validation Loss: 338.4087, Test Loss: 337.1592\n",
            "Epoch 167/200, Train Loss: 334.6093, kl :0.0410500094294548, Validation Loss: 336.2243, Test Loss: 335.6911\n",
            "Epoch 168/200, Train Loss: 334.5931, kl :0.0332532674074173, Validation Loss: 339.0430, Test Loss: 337.5188\n",
            "Epoch 169/200, Train Loss: 334.6786, kl :0.0385848730802536, Validation Loss: 339.1740, Test Loss: 337.6297\n",
            "Epoch 170/200, Train Loss: 334.5487, kl :0.038129180669784546, Validation Loss: 475.2916, Test Loss: 430.3004\n",
            "Epoch 171/200, Train Loss: 334.6100, kl :0.05492212995886803, Validation Loss: 342.8800, Test Loss: 340.1606\n",
            "Epoch 172/200, Train Loss: 334.4346, kl :0.03722730651497841, Validation Loss: 490.5734, Test Loss: 437.0492\n",
            "Epoch 173/200, Train Loss: 334.4900, kl :0.039335962384939194, Validation Loss: 487.9923, Test Loss: 443.4690\n",
            "Epoch 174/200, Train Loss: 334.4872, kl :0.052017562091350555, Validation Loss: 445.0034, Test Loss: 421.3829\n",
            "Epoch 175/200, Train Loss: 334.7797, kl :0.050542719662189484, Validation Loss: 341.1493, Test Loss: 339.2114\n",
            "Epoch 176/200, Train Loss: 334.5834, kl :0.04017099365592003, Validation Loss: 339.8065, Test Loss: 338.1673\n",
            "Epoch 177/200, Train Loss: 334.6072, kl :0.0389767549932003, Validation Loss: 478.5782, Test Loss: 440.7293\n",
            "Epoch 178/200, Train Loss: 334.6123, kl :0.03965543955564499, Validation Loss: 479.2320, Test Loss: 461.1735\n",
            "Epoch 179/200, Train Loss: 334.7184, kl :0.030536586418747902, Validation Loss: 482.2737, Test Loss: 476.0151\n",
            "Epoch 180/200, Train Loss: 334.5321, kl :0.038655225187540054, Validation Loss: 337.0606, Test Loss: 336.2832\n",
            "Epoch 181/200, Train Loss: 334.3315, kl :0.04073261097073555, Validation Loss: 480.7665, Test Loss: 427.9419\n",
            "Epoch 182/200, Train Loss: 334.5415, kl :0.03193613514304161, Validation Loss: 459.8969, Test Loss: 423.0699\n",
            "Epoch 183/200, Train Loss: 334.3981, kl :0.03975861147046089, Validation Loss: 338.1639, Test Loss: 336.9501\n",
            "Epoch 184/200, Train Loss: 334.5804, kl :0.03348107263445854, Validation Loss: 338.2259, Test Loss: 336.7674\n",
            "Epoch 185/200, Train Loss: 334.4302, kl :0.04368872940540314, Validation Loss: 459.3630, Test Loss: 430.6768\n",
            "Epoch 186/200, Train Loss: 334.4293, kl :0.02872272953391075, Validation Loss: 343.8449, Test Loss: 340.0286\n",
            "Epoch 187/200, Train Loss: 334.3761, kl :0.034159451723098755, Validation Loss: 461.0724, Test Loss: 439.1537\n",
            "Epoch 188/200, Train Loss: 334.3406, kl :0.032395340502262115, Validation Loss: 456.1974, Test Loss: 429.3201\n",
            "Epoch 189/200, Train Loss: 334.3896, kl :0.03362066671252251, Validation Loss: 340.1201, Test Loss: 337.0727\n",
            "Epoch 190/200, Train Loss: 334.5093, kl :0.039062872529029846, Validation Loss: 413.3599, Test Loss: 411.7111\n",
            "Epoch 191/200, Train Loss: 334.4137, kl :0.03610898181796074, Validation Loss: 342.7574, Test Loss: 337.2946\n",
            "Epoch 192/200, Train Loss: 334.2888, kl :0.03289797902107239, Validation Loss: 337.4004, Test Loss: 336.6235\n",
            "Epoch 193/200, Train Loss: 334.3224, kl :0.033646997064352036, Validation Loss: 486.5819, Test Loss: 475.1350\n",
            "Epoch 194/200, Train Loss: 334.4104, kl :0.042421989142894745, Validation Loss: 337.2337, Test Loss: 336.2639\n",
            "Epoch 195/200, Train Loss: 334.2973, kl :0.03751135617494583, Validation Loss: 462.9586, Test Loss: 434.4022\n",
            "Epoch 196/200, Train Loss: 334.3978, kl :0.038954753428697586, Validation Loss: 337.0219, Test Loss: 336.3706\n",
            "Epoch 197/200, Train Loss: 334.3907, kl :0.03233378753066063, Validation Loss: 468.3776, Test Loss: 434.9438\n",
            "Epoch 198/200, Train Loss: 334.3325, kl :0.03408459573984146, Validation Loss: 338.8074, Test Loss: 337.7858\n",
            "Epoch 199/200, Train Loss: 334.3204, kl :0.02935367450118065, Validation Loss: 458.2720, Test Loss: 433.2136\n",
            "Epoch 200/200, Train Loss: 334.2155, kl :0.06869431585073471, Validation Loss: 343.1568, Test Loss: 339.1462\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Loss</td><td>▃▃▃▃▃▁▁▃▁▅▁▁▁▅▁▁▁▁▁▅▁▁▅▆▅▁▁▁▄▁▅▁▁█▁▆▁▆▁▆</td></tr><tr><td>Test anmaly</td><td>███▆▆▅▇▇▆▅▆▆▄▄▅▃▃▃▃▅▃▃▄▃▅▂▂▂▄▃▃▄▂▅▁▁▃▁▁▁</td></tr><tr><td>Test kl</td><td>▁▁▁▃▃▁▁▁▁▃▃▁▄▅▁▁▄▁▅▄▅▁▁▄▁▁▁▅▅▁▁▁▁▁▁▄▁▁█▅</td></tr><tr><td>Train Loss</td><td>█▆▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>▂▃▁▃▂▃▃▁▁▂▅▁▁▄▁▄▁▁▁▁▁▁▅▅▅▁▁▁▁▅█▆▇▁██▁▇█▁</td></tr><tr><td>anmaly</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂▂▂▂▂▁▁▁▁</td></tr><tr><td>kl</td><td>▁▁▃▃▃▃▃▁▁▁▆▃▅▁▁▁▁▅▁▁▁█▂▁▅▁▁▁▆▆▁▁▅█▇▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Loss</td><td>339.14624</td></tr><tr><td>Test anmaly</td><td>0.01689</td></tr><tr><td>Test kl</td><td>4.63207</td></tr><tr><td>Train Loss</td><td>334.21554</td></tr><tr><td>Validation Loss</td><td>343.15682</td></tr><tr><td>anmaly</td><td>0.01339</td></tr><tr><td>kl</td><td>9.17836</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">daily-fire-53</strong> at: <a href='https://wandb.ai/mhmamyr743-wew/myproject/runs/cszjw0uo' target=\"_blank\">https://wandb.ai/mhmamyr743-wew/myproject/runs/cszjw0uo</a><br> View project at: <a href='https://wandb.ai/mhmamyr743-wew/myproject' target=\"_blank\">https://wandb.ai/mhmamyr743-wew/myproject</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241226_202016-cszjw0uo/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import wandb\n",
        "\n",
        "wandb.init(project='myproject')\n",
        "\n",
        "def padded_sequences_batch(batch):\n",
        "\n",
        "    sequences = []\n",
        "\n",
        "    for sequence in batch:  # Iterate over each item in the batch\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    # Pad the sequences\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
        "    padded_sequences = padded_sequences.float()\n",
        "\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "input_shape = 602\n",
        "latent_shape = 100\n",
        "mean_logits = 0\n",
        "mean_ = 0\n",
        "splits = 1\n",
        "len_data = 100\n",
        "gamma = 0.1\n",
        "bn_switch = True\n",
        "beta = 1.0\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,collate_fn=padded_sequences_batch)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,collate_fn=padded_sequences_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=padded_sequences_batch)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VRNN(input_shape, latent_shape, mean_logits, mean_, splits, len_data, gamma, bn_switch).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.5)\n",
        "lst_lossall=[]\n",
        "lst_klall=[]\n",
        "lst_anmly=[]\n",
        "lst_muall=[]\n",
        "lst_anum_test=[]\n",
        "lst_mu_test=[]\n",
        "lst_kl_test=[]\n",
        "lst_loss_test=[]\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "\n",
        "        inputs= batch\n",
        "        inputs=inputs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, diagnostics = model(inputs , beta)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    lst_lossall.append(avg_train_loss)\n",
        "    kl=diagnostics['kl'].mean()\n",
        "    lst_klall.append(diagnostics['kl'].mean())\n",
        "    lst_anmly.append(diagnostics['accuracy'].mean())\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs= batch\n",
        "            inputs=inputs.to(device)\n",
        "            loss, diagnostics = model(inputs , beta)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    wandb.log({\"Train Loss\": avg_train_loss,\n",
        "               \"Validation Loss\": avg_val_loss,\n",
        "               \"anmaly\": diagnostics['accuracy'].mean(),\n",
        "              \"kl\": diagnostics['kl'].mean()\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs= batch\n",
        "            inputs=inputs.to(device)\n",
        "\n",
        "            loss, diagnostics = model(inputs , beta)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    lst_loss_test.append(avg_test_loss)\n",
        "    lst_kl_test.append(diagnostics['kl'].mean())\n",
        "    lst_anum_test.append(diagnostics['accuracy'].mean())\n",
        "\n",
        "    wandb.log({\"Test Loss\": avg_test_loss})\n",
        "    wandb.log({\"Test kl\": diagnostics['kl'].mean()})\n",
        "    wandb.log({\"Test anmaly\": diagnostics['accuracy'].mean()})\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, kl :{kl}, Validation Loss: {avg_val_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e13f0ed83a0d4fc3a158892f11ae028d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_991e47a0e1d04da9967e6aa230a275a9",
              "IPY_MODEL_375717516f60497f91186d4f4d37d2c7"
            ],
            "layout": "IPY_MODEL_157478c8abf1431997b12934d7a38f91"
          }
        },
        "991e47a0e1d04da9967e6aa230a275a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2733f42ed517410a84db8e1aa35c6769",
            "placeholder": "​",
            "style": "IPY_MODEL_9f4c82a3f9b44fe9b40af157eed13068",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "375717516f60497f91186d4f4d37d2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560d9a8614d14314b1726dc49bfc3c4a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0279620742cd406a86f716f6b1560e71",
            "value": 1
          }
        },
        "157478c8abf1431997b12934d7a38f91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2733f42ed517410a84db8e1aa35c6769": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f4c82a3f9b44fe9b40af157eed13068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "560d9a8614d14314b1726dc49bfc3c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0279620742cd406a86f716f6b1560e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}